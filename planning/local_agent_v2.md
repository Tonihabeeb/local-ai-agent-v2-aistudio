Executive Summary
The Professional Local AI Agent system is a comprehensive platform for intelligent automation and AI-assisted workflows running entirely on a user's local machine. It combines advanced large language models (LLMs), task automation, data integration, and a modern user interface into a single extensible application. The platform’s primary objective is to enable users to harness AI for tasks such as data analysis, document querying, email management, and custom automation, all while keeping data local and secure. By providing a suite of AI agents, tools, and an automation engine, the system empowers professionals to automate routine processes, generate content, and interact with their data through natural language. The design emphasizes modularity and local-first operation – users can run powerful LLMs on their hardware (or optionally call cloud-based AI APIs) and integrate with local files and services, ensuring privacy and control over information.
In summary, the system functions as a personal AI assistant and automation hub. Users interact through a rich web-based GUI to chat with AI agents, create automated tasks or workflows, schedule routine jobs, monitor system health, and manage data (documents, emails, etc.). Under the hood, a robust backend orchestrates AI model calls, manages a task queue, monitors resources, and coordinates between the AI agents and various tools (file system, email, web, etc.). The result is a seamless experience where complex multi-step tasks can be handled by AI – for example, reading incoming emails, summarizing them, and generating draft responses – with minimal user intervention. This master document provides a complete blueprint of all functions, features, modules, and components of the application, serving as a development guide for reproducing the entire system from scratch.
System Overview & Architecture
The Local AI Agent platform is organized into multiple layers and components that work together to deliver both user-facing features and behind-the-scenes automation. Below is a high-level architecture breakdown of the system, structured into Frontend UI, Backend API, AI orchestration layer, core infrastructure, and external services:
User Interfaces: A Reflex-based web front-end (React/TypeScript generated via Python) for interactive use, and a RESTful API (FastAPI) for programmatic access. The Reflex UI runs a frontend on port 3000 (with a Reflex backend on port 8000) and communicates with the FastAPI backend (default port 8502) over HTTP. The UI provides pages for dashboard, chat, tasks, workflows, document search (RAG), monitoring, etc., while the API exposes endpoints for all functionalities (with interactive docs via OpenAPI). Communication between the UI and backend happens via HTTP calls for requests and WebSockets for real-time updates, and the API uses JSON over REST.
Application Layer: This layer contains the core logic of the system. It includes an Automation Engine for scheduling and triggering tasks, an Agents framework for AI-driven task execution, and various Services providing feature-specific capabilities. The Automation Engine comprises a scheduler for timed tasks, an event bus for event-driven triggers, and a workflow manager for multi-step task sequences. The Agents subsystem includes LLM-based agents (using SmolAgents and Microsoft’s Autogen framework) which can perform complex tasks by calling tools (RAG search, file I/O, email actions, etc.) on the user’s behalf. Additional services under this layer handle domain-specific functions like document retrieval (RAG service), email integration, prompt engineering utilities, and workflow definitions.
Core Infrastructure: The foundation includes the database, caching, and monitoring facilities. All application state (tasks, schedules, workflow definitions, execution logs, etc.) is stored in a local SQLite database (with Write-Ahead Logging for concurrency). An in-memory caching layer speeds up frequent operations (e.g. recent query results or loaded configurations). Monitoring components track system health and metrics – including resource utilization (CPU, memory, GPU), execution performance stats, and error logs – to support both the UI’s monitoring dashboard and external metrics scraping. Logging is centralized here as well, with structured logs written to files (rotated daily) and optional alerting for critical issues.
External Services: While the system runs locally, it can interface with external AI and data services as needed. For language model processing, it supports both local models (via the Ollama server for running LLMs on-device) and cloud-based models (via OpenAI or Hugging Face APIs). The OpenAI/HuggingFace integrations remain cloud-based – meaning if the user selects those, the system will call out to the respective API endpoints over the internet, whereas using Ollama allows fully local inference. The platform also integrates with email servers using IMAP/SMTP for cross-platform email automation (the design originally allowed a Windows Outlook COM interface and Microsoft Graph API, but it defaults to a generic IMAP approach for broad compatibility). Other external connectors include web services for search (e.g. DuckDuckGo for web queries) and optional cloud data stores or APIs if configured. Importantly, the system is configured such that only one model provider is active for a given request – the user can choose either a local LLM or a cloud API at a time for AI generation, ensuring that model execution for a single task happens through one pipeline or the other (not both concurrently).
Deployment Layout: In a typical deployment, all components run on the user’s local machine. The FastAPI backend (automation engine and AI orchestrator) runs as a local service (for example, via Uvicorn on port 8502), and the Reflex front-end is served on a local web server (ports 3000 for the client and 8000 for Reflex’s backend state API). The Ollama LLM server runs as a separate local process (default port 11434) to serve model inferences. During development, these can be started as separate processes (e.g. using a script to launch FastAPI and Reflex servers). For production distribution, the system can be packaged as a standalone executable for convenience, allowing end-users to install and run it with one click. This deployment kit would bundle the Python environment, backend, front-end, and model files into an .exe (using tools like PyInstaller or similar) so that non-technical users can launch the application without manual setup. (Alternatively, a containerized deployment via Docker Compose is possible – splitting the UI and API into separate containers behind a reverse proxy – but the primary target is a local installable application.)
Frontend (Reflex UI) Specification
The frontend is a Reflex web application (Reflex was formerly Pynecone) that provides a rich, reactive user interface. It is essentially a React-based single-page app generated by Python code, offering real-time interaction with the backend. The UI is organized into multiple pages/tabs, each corresponding to a key feature of the system. Below is an overview of all user-facing pages and their functionality:
Dashboard: The landing page (/dashboard) gives a high-level overview of system status and quick stats. It might display summaries such as the number of tasks, upcoming scheduled events, and a live “health bar” indicating system resource usage. This page serves as a control panel with navigation shortcuts to other sections.
Chat: The chat interface (/chat) allows the user to have a direct conversation with an AI assistant. Users can enter prompts or questions and receive AI-generated responses in a streaming fashion. The chat UI supports rich text formatting – for example, code blocks for programming answers and reference citations when using RAG (Retrieval-Augmented Generation) mode. The user can adjust the model (select from local or cloud models, e.g. a dropdown for “Local (Ollama)” vs “Cloud (OpenAI)”) and parameters like temperature. Under the hood, each chat message is sent to the backend via an API call, which invokes the LLM, and the response is displayed. The chat page may also include conversation history (with the ability to clear history) and features like copy-to-clipboard for answers or formatting options for better readability.
Tasks (Agent Console): The tasks page (/tasks) is a management console for automation tasks. It lists all available tasks (custom Python scripts or “agents” that the user has created) and provides controls to run them or view details. A key feature here is the AI Task Builder – the user can describe a new task in natural language (for example, “Summarize my weekly emails into a report”) and the system’s AI-assisted task creator will generate a YAML plan and Python code template for that task. This involves calling an AI endpoint to draft a task outline (including required tools and steps), which the user can then refine. The page likely displays task metadata (name, description, last run status) and allows creating, editing (possibly opening the code in an editor), and deleting tasks. When a task is executed (either manually or via automation), its progress might be reflected here or in the Executions page.
Workflows: The workflows page (/workflows) provides a visual workflow builder and list of multi-step workflows. A workflow is a sequence or directed graph of tasks and actions that run in a defined order (possibly with branching or conditional logic). In the UI, this could be represented as a DAG (directed acyclic graph) or a series of steps – the Reflex UI includes custom components for a workflow diagram editor. Users can create a new workflow by specifying a series of tasks to execute and linking them. The page likely shows existing workflows, allows editing them (adding nodes, connecting arrows), and deploying or running them. Workflows are executed by the backend orchestrator ensuring each step’s output can feed into the next step’s input.
Automation (Schedules & Events): The automation page (perhaps /automation) consolidates management of scheduled tasks and event triggers. In the Schedules section, users can create and view scheduled jobs – for example, “Run Task X every day at 9:00 AM.” Each schedule includes a cron expression or interval, the task to run, and status (enabled/disabled, last run, next run). The UI allows creating new schedules (with forms for time/frequency), editing existing ones, or canceling schedules. In the Events section, users manage event-driven triggers. These are rules like “When Event Y occurs, execute Task Z.” Events can be things like a new file added to a directory, an incoming email, or a custom webhook trigger. The UI provides forms to define an event subscription (choosing event type and specifying filters or conditions, e.g., file path or email folder, etc.) and map it to a task or workflow. This page provides an overview of all active automations, so the user can see how the system will react automatically to time or events.
Agents (Multi-Agent Teams): The agents page (/agents) is for managing multi-agent configurations and agent settings. In advanced use cases, the system can deploy multiple AI agents that collaborate (for instance, using Microsoft’s Autogen framework with roles like “Researcher”, “Coder”, “Critic”, “Executor”). This page would allow the user to initiate or configure such multi-agent sessions. For example, the user could provide a complex problem and launch a team of agents to discuss or tackle it together. The interface might show each agent’s role and messages in a collaborative chat or a timeline of interactions. It also likely lists any saved agent configurations (like predefined teams or specialized agent personas) and allows enabling/disabling specific agent frameworks (since multi-agent mode can be resource-intensive). Agent management could include selecting which tools or knowledge each agent has access to, setting their objectives, or terminating a multi-agent session.
RAG (Retrieval-Augmented Generation) Center: The RAG Center page (/rag) is a feature-rich interface for document interaction. It allows users to upload documents (PDFs, text, Word docs, etc.), index them into a vector database, and then chat with the content of those documents. The page is typically divided into three panels:
Left Panel – Document Browser: showing the list of documents/folders in the indexed library, with indicators of indexing status (e.g., a checkmark if indexed successfully). Users can drag-and-drop files here to add to the corpus, search or filter documents by name, and perform bulk operations like re-indexing or deletion.
Center Panel – Chat Interface: similar to the Chat page, where the user asks questions or requests based on the documents. The AI will answer by pulling information from the relevant files. Responses are streamed token-by-token, and citations are provided for factual statements with references to the source documents. The chat may also display the AI’s confidence or allow follow-up questions. Users can export the conversation (for record-keeping) and see suggested follow-up queries.
Right Panel – Document Viewer: when the AI answer includes citations, the corresponding documents’ content is displayed with highlights on the cited passages. This panel lets the user read the context in the source file (with perhaps multiple colors to highlight different cited sources), scroll through documents, and even annotate or copy text. It synchronizes with the chat: clicking a citation in the answer scrolls the document viewer to the relevant excerpt.
The RAG Center thus tightly integrates the vector search capability with the chat UI, enabling an experience where the AI acts like a knowledgeable assistant grounded in the user’s own documents. The front-end handles file uploads by sending them to the backend for storage (and later indexing), and it calls search APIs to retrieve relevant snippets to display or feed into the LLM’s context.
Monitoring: The monitoring page (/monitoring) displays real-time system health and performance metrics. This includes charts and gauges for CPU usage, memory usage, GPU utilization (if available), disk space, and possibly network or API call statistics. Using libraries like Plotly, the UI can render graphs of resource usage over time. The page likely shows the “Health Bar” or status indicators that are also present in the header (for quick glance of CPU/GPU stats). It may also list running tasks or background jobs, and show any system alerts (for example, low disk space warnings or high memory usage alerts). The monitoring page obtains data via WebSocket subscriptions or periodic API calls to the health endpoints, enabling live updates without full page refresh. Users use this page to ensure the AI system is running smoothly and to identify any bottlenecks (e.g., if an LLM is using too much memory or if a scheduled task is consuming high CPU).
Mail (Email Client with AI features): The mail page (/mail) provides an email client interface within the application, designed to look and feel like a simplified Outlook-style mail reader. It features a three-pane layout: a list of mail folders (Inbox, Sent, Drafts, etc.), a message list for the selected folder (with toggles for viewing only unread emails, etc.), and a reading pane showing the contents of the selected email. Users can connect the system to their email account via IMAP (entering their mail server, username, and password in a configuration). Once connected, the Mail page will fetch emails from the server and display them. The unique aspect is integration of AI features: for instance, the user can click a button to have the AI summarize a long email thread, or draft a reply automatically. When drafting replies, the interface could open a compose window with a draft message written by the AI (in Dry-Run Mode, meaning it’s not sent automatically – the user can review and manually send through their email server). The system can also analyze email content (e.g., detect sentiment or extract action items using the AI). Under the hood, when the user requests an AI action (like “Summarize this email” or “Draft a reply”), the request goes to the backend which uses an LLM (and possibly context from relevant emails) to generate the output, which is then displayed for the user. The mail integration does not require Outlook to be installed – it uses IMAP for retrieving emails and SMTP for sending/drafting, which makes it cross-platform. (Previously, a Windows-only Outlook COM integration was available, but the current design favors IMAP for universal support. The UI remains similar to Outlook, but all interactions go through the IMAP connector.)
Executions: The executions page (/executions) shows a history of task and workflow runs (the execution log). Every time a task or workflow is executed (whether manually from the Tasks page or automatically via schedule/event), a record is stored in the database. This page lists those records, including details like timestamp, the task/workflow name, status (success, failure, running), duration, and any outputs or results. The user can click an execution entry to see more details – for example, a step-by-step log of what the agent did, what tools it invoked, and any error trace if it failed. This is invaluable for debugging tasks and understanding agent behavior. The UI might allow filtering executions by date or task name, and possibly retrying a failed execution or viewing artifacts produced (files, reports, etc.). It pulls this data from the backend /api/v1/executions endpoints which query the database for execution history.
Logs: The logs page (/logs) provides a view into system log files for advanced debugging. This likely reads from the application’s log files (e.g., logs/agent\_system.log and logs/agent\_system\_errors.log) and displays recent log entries with timestamps and severity levels. It may allow the user to filter by log level (info, warning, error) or search within logs for specific messages. This page is mostly for developers or power users to troubleshoot any issues not evident from the UI alone. It likely uses an API endpoint to read a portion of the log file or tail it in real-time.
Component Hierarchy and State Management: The Reflex UI is built with a structured component hierarchy, leveraging a design system for consistency. At the top level, an AppShell layout component provides the common navigation sidebar, header, and footer across all pages (ensuring things like the breadcrumb, theme toggle, and version info are consistently shown). Each page is a component (e.g., dashboard.py, tasks.py, chat.py, etc.) composed of smaller reusable components (like tables, cards, form inputs, buttons, modals, etc.). The design system defines standard styling via tokens – colors, spacing (8pt grid), typography, etc. – which all components use for a cohesive look. This modular approach means developers can easily plug in new pages or components without breaking the design.
Reflex manages UI state on the server side using special State classes. For example, there is a global AppState that holds common state (like user authentication status, theme mode, or global loading indicators). Each major feature might have its own state class (e.g., RagState for the RAG page to keep track of selected documents and query results). State changes are triggered by user events (button clicks, form submissions) and by backend push events. Reflex uses an async event loop and WebSocket connections to keep the UI in sync: when the backend state changes (say a new execution log is added, or a health metric updates), Reflex broadcasts these changes to the UI in real-time. This means the UI can update live, showing (for instance) a task status moving from "running" to "completed" without the user manually refreshing.
UI-Backend Communication: Most user actions in the UI result in HTTP requests to the FastAPI backend via the Reflex backend. For example, when a user clicks "Run Task" on the Tasks page, the UI calls an API endpoint (e.g., POST /api/v1/tasks/{id}/execute) through an api\_client service in the Reflex app. The Reflex app uses httpx for asynchronous HTTP calls to the FastAPI server. Similarly, uploading a document from the RAG page calls the /api/v1/rag/documents/upload endpoint with the file stream. For real-time updates, Reflex’s built-in WebSocket connection is used: the backend (Reflex side, not FastAPI) can push state changes to the client. The design is such that long-running processes (like an AI generating a response or a task executing) are handled by the FastAPI backend asynchronously; once the result is ready or intermediate progress is available, the Reflex state is updated (for instance, via a polling coroutine or notification) and sent to the front-end via WebSocket. This architecture avoids constant polling on the client and ensures the UI is responsive and up-to-date.
Dependencies and Libraries (Frontend): The front-end relies on several key libraries: - Reflex – the core framework that translates Python UI code into a reactive web app, handling routing, state, and WebSocket comms. - Plotly – used to render interactive charts/graphs on the Monitoring page (for example, plotting GPU utilization over time). - Websockets – utilized by Reflex for real-time updates (the Reflex backend establishes WebSocket connections to each client for state updates). - HTTPX – an async HTTP client used within Reflex server-side code to call the FastAPI API routes for any action that requires backend data or mutation. - Aiofiles – used for efficient async file operations (for instance, when uploading or downloading files in the UI, such as saving an uploaded document to disk, or serving file content to the browser). - pytest (and pytest-asyncio) – while not used in the running app, these are included to write and run tests for the UI logic and state management, ensuring the front-end components and API integrations work as expected during development.
The UI is designed with styling and structure for plug-and-play modularity. This means new pages or components can be added following the same patterns without impacting existing functionality. All styles are derived from the centralized design tokens (no hard-coded colors or pixel values), making it easy to maintain consistency. The component library (buttons, inputs, tables, etc.) is reusable, so developers don't need to rewrite common UI elements. Navigation is configured in one place (a global NAV\_CONFIG), so adding a new page involves creating the page component and updating the nav configuration and sidebar menu. This consistent architecture ensures the front-end remains maintainable and scalable as the system grows.
Backend (FastAPI Runtime) Specification
The backend is a FastAPI application that serves as the brains of the Local AI Agent system. It exposes a comprehensive REST API (under the prefix /api/v1/...) and implements all the core logic for task execution, scheduling, AI model interaction, and data management. The FastAPI app is initialized in api/app.py with metadata like title and version, and is configured with CORS middleware to allow the Reflex UI (running on a different port) to communicate with it. The API is documented via automatic OpenAPI generation (accessible at /api/docs or /api/redoc in a development setting).
Endpoints and Routers: The backend organizes its endpoints into several routers, each handling a distinct set of features. Below is a detailed list of all major API endpoints (grouped by router) and their purpose:
System Status Endpoints:
GET /api/v1/health – Returns a comprehensive health check of the system. This endpoint gathers details such as database connectivity, available disk space, memory usage, uptime, and checks the status of key components (e.g., whether the Ollama LLM server is reachable, whether the scheduler and event bus are running properly). The response is a JSON with nested fields for each component (e.g., database: OK, ollama: OK, memory\_percent: 70%, automation: { ... } etc.). This powers the UI health bar and can be used by external monitors to ensure the service is operational.
GET /metrics – Provides Prometheus-compatible metrics output. This includes counters and histograms such as number of requests, error counts, average response times, and custom metrics like number of tasks executed, etc. It’s intended for integrating with monitoring systems (Prometheus/Grafana). The endpoint outputs plain text metrics data.
Tasks Management (/api/v1/tasks): Responsible for listing, creating, and controlling tasks (the atomic units of automation).
GET /api/v1/tasks – List all available tasks. The backend scans the tasks directory or internal registry for task definitions. Each task has metadata (name, description, parameters) possibly loaded from a YAML or docstring in the task’s Python file.
POST /api/v1/tasks – Create a new task. This could accept a JSON or YAML payload defining the task, or possibly code. In practice, tasks might be created via file creation (through the AI Task Creator or user uploading a script), so this endpoint might not be heavily used, but it’s part of the API.
GET /api/v1/tasks/{task\_id} – Get details of a specific task (including its code or plan).
POST /api/v1/tasks/{task\_id}/execute – Execute a task immediately. This triggers the automation engine to run the task asynchronously. The endpoint returns an acknowledgment (maybe an execution ID to track status).
Additional sub-endpoints may handle enabling/disabling tasks or updating them.
Schedules (/api/v1/schedules): CRUD for scheduled tasks.
GET /api/v1/schedules – List all scheduled jobs (with their cron, next run time, associated task, etc.).
POST /api/v1/schedules – Create a new schedule (with a cron expression or interval and a target task id).
PUT /api/v1/schedules/{schedule\_id} – Update a schedule (e.g., change timing or toggle active state).
DELETE /api/v1/schedules/{schedule\_id} – Remove a scheduled job.
When a schedule is created or modified, the backend uses APScheduler to add or update the corresponding job in the scheduler, ensuring the task will be queued at the specified times.
Events (/api/v1/events): Manage event-driven triggers.
GET /api/v1/events – List all event subscriptions (each subscription might include the event type, filter conditions, and the task/workflow to trigger).
POST /api/v1/events – Create a new event subscription. For example, a payload could specify event\_type: "file\_created" and filter: {"directory": "reports/"} and an action like task: "generate\_summary". The backend will store this in the event\_subscriptions table and register it with the event bus.
DELETE /api/v1/events/{event\_id} – Cancel an event subscription.
Executions (/api/v1/executions): Access execution records and logs.
GET /api/v1/executions – List past execution records (with filtering options such as status=failed or date range).
GET /api/v1/executions/{exec\_id} – Get detailed info and log of a specific execution (including each step the agent took, any errors, and links to artifacts produced).
Possibly endpoints to retrieve artifacts or outputs of executions (if they are saved as files).
Workflows (/api/v1/workflows): Manage multi-step workflows.
GET /api/v1/workflows – List defined workflows (each might have an ID, name, and description).
POST /api/v1/workflows – Create a new workflow by providing its definition (sequence of tasks, conditions).
GET /api/v1/workflows/{workflow\_id} – Get details of a workflow (including its steps).
PUT /api/v1/workflows/{workflow\_id} – Update a workflow definition.
DELETE /api/v1/workflows/{workflow\_id} – Remove a workflow.
POST /api/v1/workflows/{workflow\_id}/execute – Run a workflow immediately. This will enqueue the workflow’s first step and the engine will manage subsequent steps.
Chat (/api/v1/chat): Endpoints for the chat functionality (conversational AI).
POST /api/v1/chat/message – Send a chat message from the user and get an AI response. The request includes the user's message and optionally the model to use and generation parameters. The backend will forward this to the LLM (either via local Ollama or an external API depending on configuration). In the current implementation, this route calls the Ollama server’s API to generate a response. The response is returned once the AI has completed, containing the assistant's reply text. (Note: streaming responses, if enabled, might use a different mechanism such as WebSocket or server-sent events; the current code sets stream: False meaning it waits for the full response before returning).
GET /api/v1/chat/history – Retrieve the recent chat history. This allows the UI to load past messages (currently stored in memory or could be persisted in DB in future).
DELETE /api/v1/chat/history – Clear the chat history. Resets any stored conversation context.
RAG (Retrieval-Augmented Generation) (/api/v1/rag): Document indexing and query endpoints.
GET /api/v1/rag/documents – List all documents that have been added to the system’s docs library, along with metadata. This scans the ./docs directory and returns file info (filename, relative path, size, last indexed time) for each supported document type. It does not necessarily confirm if each is indexed, but presence implies the user can index/search them.
POST /api/v1/rag/documents/upload – Upload a new document file for indexing. The file is received as form-data, saved to the docs/ folder on disk, and a response is returned confirming the upload. (The actual indexing – embedding the document text into the vector store – might be triggered separately, e.g., the user runs an indexing routine or the system watches the docs folder for new files).
POST /api/v1/rag/search – Query the document index for relevant content. The request includes a text query and a parameter top\_k for number of results. The backend uses the vector search pipeline to find the most similar chunks of text to the query. Internally this calls a query\_rag or similar function which loads the FAISS index and performs similarity search using an embedding of the query. The results (a list of snippets with source file references) are returned to the caller. If the RAG system is not configured or an error occurs, an appropriate error or an empty result is returned. The UI uses this to either show the search results or feed them into an LLM to get a final answer.
DELETE /api/v1/rag/documents/{doc\_id} – Remove a document from the index/library. (This might currently be a stub or placeholder – actual deletion would remove the file and update indexes.)
AI Assistance & Generation: There are a set of endpoints where the AI helps generate content for the user – these serve the AI Task/Workflow builder features.
POST /api/v1/task\_generation – Given a natural language description of a desired task, the backend LLM will generate a structured Task specification (potentially a YAML or JSON outlining the steps and tools, plus a skeleton Python code). This endpoint leverages an agent or prompt template to interpret the user's request and produce code. The result is then returned (and the UI can present it for confirmation before creating the actual task file).
POST /api/v1/schedule\_generation – Similar concept but for schedules. The user might say "Every Monday at 9am do X" in plain English; this endpoint would parse that (with AI) into a cron schedule and task mapping which can then be saved as a Schedule entry.
POST /api/v1/event\_generation – For events, the user could describe a trigger in natural terms ("When a file is added to reports folder, run the summary task"). The AI will attempt to translate this into an event subscription specification (identifying the event type, filter, and action).
These generation endpoints use LLMs (via LangChain or direct prompt engineering) to assist the user in configuration, making the system more user-friendly by removing the need to manually write YAML or code for common setups.
AI Governance (/api/v1/governance): Endpoints under this might relate to a higher-level management of AI behavior. For example, the system could allow retrieving or setting guardrails, reviewing the chain-of-thought of agents, or controlling global AI settings. If an AI Governance Engine (using LangGraph) is implemented, routes here could expose its functions:
POST /api/v1/governance/validate\_plan – Given a task plan or agent output, the governance module validates it against policies.
GET /api/v1/governance/rules – List current rules or policies in effect.
(Speculative; the actual routes depend on implementation. The architecture mentions an AI Governance Engine but specifics might be internal.)
Multi-Agent Collaboration (/api/v1/multi\_agent): These endpoints manage sessions where multiple AI agents converse or collaborate on a problem.
POST /api/v1/multi\_agent/session – Start a new multi-agent session with a given prompt or goal. This might instantiate the Autogen framework with pre-defined roles (e.g., a researcher agent and a critic agent, etc.). The backend would then orchestrate a dialogue between these agents (each agent is essentially an LLM with a role profile). The response could be the final outcome or solution the agents converge on.
GET /api/v1/multi\_agent/session/{id} – Fetch the status or transcript of an ongoing multi-agent session. Possibly returns the messages exchanged between agents and any partial result.
POST /api/v1/multi\_agent/session/{id}/step – (If not fully automated, maybe allows nudging the conversation or adding a human message in between.)
In essence, these routes facilitate experiments where AI agents talk to each other to solve a task, and provide the UI the necessary hooks to display those interactions.
Learning (Continuous Improvement) (/api/v1/learning): If implemented, these endpoints might provide insights and suggestions for improving tasks or system usage. For example:
GET /api/v1/learning/insights – Returns analysis of recent task executions, detecting failures or inefficiencies and suggesting optimizations. The “Learning Engine” might use an LLM to read through execution logs and propose changes (like refactoring a task, adjusting a prompt for better results, or scheduling at a different time due to observed performance issues).
POST /api/v1/learning/feedback – Accept feedback on tasks or AI responses (maybe to fine-tune prompts or record human corrections).
This component is forward-looking – enabling the system to semi-autonomously refine its behavior over time.
Config (/api/v1/config): Provides access to the system configuration at runtime.
GET /api/v1/config – Read current configuration settings (as a consolidated JSON of config.yaml and environment-derived settings, excluding secrets).
POST /api/v1/config – Update certain configuration values on the fly (for instance, toggling a feature flag like enabling the multi\_agent mode, or changing the model temperature default).
These allow the UI’s settings page to display and modify configuration without needing a restart or manual editing of files.
Outlook/Email (/api/v1/outlook): Endpoints to interact with email. (Named "outlook" in code but conceptually covers IMAP email integration.)
GET /api/v1/outlook/folders – List email folders/mailboxes available (Inbox, Drafts, etc.) by connecting to the configured IMAP server.
GET /api/v1/outlook/folder/{name} – Get a list of messages (with basic info like subject, sender, date, unread status) in the specified folder. Supports query params for pagination or filtering (e.g., ?unread\_only=true).
GET /api/v1/outlook/message/{id} – Retrieve the full content of a specific email (with HTML body).
POST /api/v1/outlook/message/{id}/reply – Generate a draft reply to the specified email. This triggers the AI to read the email content and compose a suggested reply. The reply text is returned (and optionally saved as a draft via SMTP or just provided to UI for the user to copy).
POST /api/v1/outlook/send – (If sending is allowed) send an email via SMTP with given recipients, subject, body (this would be used after user finalizes a draft).
Internally, these endpoints use an email client library to talk to the IMAP server configured in environment (host, user, pass). They also utilize the AI model for things like summarization or drafting. For instance, when generating a reply draft, the endpoint may fetch the original email content, then prompt the LLM with something like: "Based on this email, draft a polite and concise reply." The result is returned but not auto-sent, respecting user control.
Each of these endpoints has request/response schemas defined using Pydantic models (as seen for ChatMessageRequest/Response, RAGSearchRequest/Response, etc.). This enforces data validation – if the client sends invalid data, FastAPI will return a clear 400 error with details wrapped in an ErrorResponse schema (the code globally uses an exception handler to format errors consistently).
Backend Runtime Logic and Middleware: The FastAPI app includes middleware components for enhanced functionality: - Security Headers Middleware: Adds standard security HTTP headers on all responses (e.g., Content-Security-Policy, X-Frame-Options, etc.). - Error Handling Middleware: Catches unhandled exceptions and formats them as JSON error responses rather than plain stack traces. This improves API clarity. - Logging Middleware: Logs each request and response (including execution time and status) for audit purposes. This uses Python’s logging configured to write to log files. - Rate Limiting Middleware: Integrated via SlowAPI (Flask-Limiter adaptation for Starlette) to prevent abuse. The configuration might set a global rate like 200 requests per minute per IP. If exceeded, a 429 Too Many Requests is returned. This is important especially if the UI or external clients rapidly poll certain endpoints.
Additionally, the app supports authentication: a simple bearer token scheme (for local usage, this is optional). If enabled (via an environment variable like API\_AUTH\_TOKEN), many endpoints use Depends(verify\_token) to ensure the request carries the correct token. The UI would store this token (perhaps in Reflex state) and include it in API calls. This prevents unauthorized external access if the machine is networked.
Background Task Scheduling (APScheduler): The backend launches an APScheduler background scheduler at startup to handle scheduled tasks and periodic maintenance jobs. All schedules defined by the user (in the database) are loaded into APScheduler as jobs. For example, if a task is scheduled daily at 9am, APScheduler will call a job function at that time, which enqueues the task execution into the system. The scheduler likely runs in a separate thread or the event loop, and is configured with an appropriate executor (e.g., AsyncIOExecutor or default threading). APScheduler is also used for system health tasks: for instance, one might schedule a job to periodically record metrics (like writing CPU usage to the metrics table every minute) or to cleanup old logs. The config file can specify any recurring jobs (like a daily maintenance task). Because FastAPI’s event loop can run background tasks, APScheduler is the mechanism to achieve cron-like functionality within the app’s lifecycle.
GPU Monitoring (GPUtil) and System Health (psutil): The backend uses GPUtil to query GPU statistics (if an NVIDIA GPU is present) – such as memory used, utilization percentage, temperature, etc. These are either served on the health endpoint or via a dedicated monitoring endpoint. Similarly, psutil provides CPU load, RAM usage, disk usage, and system uptime. On each health check call (or perhaps on a schedule), the backend collects these metrics and may store some in an in-memory cache or the database. This data is what powers the UI charts on the Monitoring page. The architecture is such that the UI could also subscribe to continuous updates (though simplest implementation is that the UI polls the health API periodically, say every few seconds, to get updated metrics). The system health tracker also monitors sub-systems: for example, it may check that the Ollama LLM server is responsive by calling a quick endpoint (like asking for model list) and include that status. If any component is not healthy, the health check will mark it and possibly include an error message for the UI to display. This comprehensive health approach ensures the user knows if something needs attention (e.g., "Ollama not running" or "Low Disk Space" warnings).
Service Orchestration and AI Engine Integration: At the heart of the backend lies the orchestration logic that ties together the LLMs, tools, and tasks: - The system uses SmolAgents as the primary agent framework. SmolAgents provides a lightweight way to define AI agents that can use tools and produce outputs (similar to LangChain agents but more streamlined). The backend likely has a factory (in smol\_agent\_factory.py) that, given a task definition, creates a configured agent instance. SmolAgents is OpenAI-compatible, meaning it can interface with any model that speaks the OpenAI chat completions API (which Ollama also mimics). The agent can call tools when needed during its execution. - For multi-agent scenarios, the system integrates Microsoft’s Autogen (pyautogen) which is referred to as “Microsoft Agent Framework” in the prompt. Autogen enables multiple LLM agents to have a conversation or a cooperative session. The backend’s multi\_agent router uses this to set up agents with distinct roles and have them exchange messages autonomously until a solution is reached or a stopping condition. - LangChain is included as well (with langchain-core, langchain-community packages). LangChain might be used for certain utilities like prompt templates, chaining sequences (particularly for the AI task generation or for handling the RAG combination of retrieval + generation). It could also provide integration for memory management or additional vector store abstraction if needed. However, the design opts for SmolAgents for the core agent loop due to its simplicity and robust tool system. - LangGraph is another library present, which supports building conversational or reasoning flows as graphs. The AI Governance Engine in the system might use LangGraph to enforce structure in the agent’s reasoning. For example, LangGraph can define nodes that represent steps in a reasoning chain, and ensure the agent follows a certain path (maybe first retrieve info, then analyze, then answer). The backend’s governance router and ai\_gov module implement this. Essentially, the AI Governance Engine can wrap around agent execution to apply additional checks or guide the sequence of tool calls, ensuring compliance with logic or policies. For instance, it might intercept an agent’s plan and validate it (using jsonschema for structured output validation) or prevent certain actions unless approved.
Tool Integration: A variety of tools are available to the agents, enabling them to perform actions beyond text generation. These tools include:
RAGSearchTool: which allows an agent to query the document index for relevant info (this wraps the RAG search functionality – effectively calling the same logic as /api/v1/rag/search – and returns snippets to the agent). An agent can use this tool if it needs information from the user’s documents during a task.
File System Tools: such as FileReadTool (to read the contents of a local file), FileWriteTool (to write or modify files). These let agents generate reports or code and save them to disk. There might also be a FileListTool to list directory contents.
Email Tools: like OutlookListUnreadTool, OutlookReadTool, OutlookReplyDraftTool. These allow an agent to check for new emails, read an email’s content, and create a draft reply. Under the hood, these use the email integration (IMAP for listing/reading, and the AI model for drafting). The naming “Outlook” remains from the original design, but they function via IMAP now. Using these, an automation task could, for example, automatically read unread emails and summarize them.
Office Document Tools: e.g., ExcelCreateTool or WordReportTool. These would utilize libraries like openpyxl or python-docx to allow an agent to create Excel spreadsheets or Word documents. A task might gather data and then use these tools to produce a nicely formatted report file.
Web Tools: DuckDuckGoSearchTool and VisitWebpageTool allow an agent to perform web searches and retrieve web page content (text). This is useful for agents that need external information (though in a “local” AI agent scenario, web access is optional and only done if the user permits/has internet). The ddgs library provides DuckDuckGo results, and a simple web scraper tool can fetch pages for the agent to read.
Development Tools: CursorRequestTool and CursorApplyFilesTool. These appear to be related to code generation and application. It suggests integration with Cursor (an AI coding assistant) where the agent can request changes or create code files and apply them. Possibly, the AI Task creator uses these to generate multiple files or changes in the codebase automatically.
Memory Tools: A VectorMemoryTool could be available for long-term memory. This would allow an agent to store and retrieve embeddings (using FAISS or a similar vector store) across interactions, effectively remembering information beyond a single session.
All these tools are registered with the agent frameworks. The smolagents library likely handles tool invocation by special formatting in the LLM prompt (e.g., the agent can output an action like Search["keyword"] and the system will execute the DuckDuckGo search and feed the result back). The backend orchestrator ensures that if an agent decides to use a tool, the corresponding Python function is called and results are properly passed back into the LLM’s context.
AI Task Execution Flow: Putting it together, when the user triggers a task execution (via the UI or an automation trigger), the backend follows a sequence like this: 1. Queuing: The task request is placed into a task queue or executor system. The architecture uses a thread pool or async task group to allow multiple tasks to run concurrently (with a configured max concurrency, e.g., 12 parallel tasks by default). This queue is managed by the Automation Engine (likely via automation/queue.py or directly APScheduler for immediate tasks). 2. Execution Record: A new execution entry is created in the database (in the executions table) to track this run. It records the task ID, start time, status “running”, etc. A unique execution ID is generated. 3. Agent Initialization: The system builds an AI agent for the task. This involves loading the task’s configuration (which might include a prompt, list of tools it needs, any input parameters) and using the SmolAgent factory to create an agent instance with the specified LLM model and the allowed tools. Depending on the task type, it could be a CodeAgent (allowed to execute Python code) or a ToolCallingAgent (only allowed to call tools without writing code). The agent is given access to the relevant tools (from the list above) as defined in the task’s YAML or in a default set. 4. Task Execution: The agent (which is essentially an LLM prompt loop) is run to perform the task’s steps. This may involve: - Calling the LLM to generate a plan or to decide next action. - If the LLM chooses to use a tool (say search the web or read a file), the backend intercepts that action and executes the corresponding tool function, then provides the result back to the LLM's context. - The agent might iterate through multiple steps of thinking and acting (this is typical in frameworks like SmolAgents or LangChain agents). - It might also execute embedded Python code if it's a CodeAgent (which could happen in a sandbox or directly if it's safe). - Throughout this process, each step is logged: the execution engine captures what the agent is doing at each stage (e.g., “Agent decided to use FileReadTool on XYZ.txt”, followed by “Output of FileReadTool: ...”). These logs are stored in the execution\_steps or execution\_logs table, and will be visible in the Execution history UI. - The agent will eventually arrive at a final output (could be a text result, a file created, an email sent, etc.). 5. Completion: Once the task is done, the execution record is updated to “completed” (or “failed” if an error was encountered) with an end timestamp. Any result produced is stored – e.g., if the task generated a report file, a reference to that file might be saved as an artifact record in the database. 6. Result Return: If the task was triggered by a user action waiting for a response (like a direct run), the final result or a success acknowledgment is returned via the API. If it was triggered in the background (schedule/event), there may be no direct API response to return, but the UI could be notified via WebSocket or simply the user sees it in the history next time they check. 7. Post-processing: The system might perform additional steps after completion, such as sending notifications (if configured), or in a continuous learning setup, analyzing the execution log for improvements.
This flow ensures a robust execution where even if the AI agent fails or an exception occurs, the system catches it (try/except around the agent run) and marks the execution record accordingly, including error details for the log.
Event-Driven Task Flow: When an event subscription is triggered (e.g., file created or an incoming email): 1. A sensor (like the FileSensor watching the filesystem) detects the event and calls emit\_event on the central Event Bus. The event has a type (e.g., EventTypes.FILE\_CREATED) and payload (like file path, name, etc.). 2. The Event Bus (in automation/events.py) receives this and looks up which subscriptions match this event type and payload. For instance, a subscription might specify a certain directory; the bus checks if the file path matches any subscribed directory. 3. For each matching subscription, the Event Bus submits the corresponding task or workflow to the execution queue. It may also log an entry in an events log. 4. The task then runs through the same execution flow as above (steps 2-6 of task execution flow). Because this is asynchronous, the user doesn’t directly see it unless they check the Executions page or have a notification mechanism. 5. If any output or outcome needs to be communicated (e.g., an email auto-reply was drafted), the system might send it or store it according to task logic.
The event system allows the AI agent to react to real-world triggers automatically, achieving true automation.
Background Services and Automation Subsystems: Beyond user-triggered tasks, the backend runs a few automation services continuously: - File Watcher: The mentioned FileSensor uses the watchdog library to monitor directories (by default ./docs and ./outputs) for changes. This enables features like auto-indexing new documents or triggering workflows when new output files are generated. The file sensor runs in background threads (started on app startup) and emits events for create/modify/delete/move, which the Event Bus can use to e.g. auto-index or alert the user. - Email Checker: If using IMAP and wanting event-driven email handling, the system might poll the email inbox every X minutes (since IMAP IDLE or push isn’t always straightforward). There could be a scheduled job (via APScheduler) that checks for new unread emails and emits an email\_received event which could trigger a task (like auto-summarize). With Microsoft Graph API, webhooks could be used; with generic IMAP, periodic polling is a simple solution. - System Watchdog: The system might use watchdog not just for files but to monitor other aspects (though primarily it’s for files; the rest uses direct checks via psutil/GPUtil rather than event-driven). - Backup Mechanism: The application includes scripts for backing up and restoring data (e.g., scripts/backup.sh to copy critical files like the database, config, and documents to a backup archive, and restore.sh to restore from it). Additionally, the Reflex UI has a config\_backup\_service.py which can periodically save snapshots of the configuration (perhaps before and after major changes). While not a real-time feature, these are part of maintenance – ensuring if a user screws up a config or an update fails, they can revert. The engineering plan may include instructing the scheduled backup of the SQLite DB daily, given its importance.
In sum, the backend is not just a request/response server, but a runtime environment that continually orchestrates tasks, monitors triggers, and manages resources. It is carefully structured to handle concurrency (multiple tasks, multiple user requests simultaneously), and to recover gracefully from errors (using try/except and marking failures without crashing the server).
AI & Data Layer
This section describes how AI models are managed and used, how data (documents, knowledge) flows through the system, and how information is stored.
Model Management: The system supports multiple LLM providers to give flexibility between local and cloud models. Model configuration is typically set in config.yaml under a model section and via environment variables: - Local Models (Ollama): For offline or on-prem use, the system integrates with Ollama, a local LLM server. Ollama allows running large language models on the user's hardware (leveraging GPU if available). The backend communicates with Ollama via its HTTP API (default http://localhost:11434). The user must have Ollama installed and models downloaded beforehand. Example models might include qwen2.5:7b (a general model), qwen2.5-coder:14b (a coding-focused model), etc.. These are pulled via ollama pull  prior to use. The advantage is low-latency, no internet required, and data never leaves the machine. The system’s default mode is often to use an Ollama model for all AI tasks (the Chat endpoint by default targets qwen2.5:7b local model). - Cloud Models (OpenAI/OpenRouter/HuggingFace): For cases where internet is available or more powerful models are needed, the system can call cloud APIs. This includes OpenAI’s API (for GPT-4, etc.) and Hugging Face models via endpoints. The integration is often done through OpenRouter (a proxy that provides unified access to many free and open models via an API key). If the user provides an OpenRouter API key in the .env and enables it in config, the system can route requests to OpenRouter’s cloud models. Through this, models like “Gemini 2.0” or “Qwen3 480B” become available (as noted in the documentation). Alternatively, direct OpenAI API usage is supported if an OPENAI\_API\_KEY is provided – SmolAgents and LangChain can call OpenAI’s chat completions with that key. Hugging Face Hub usage (e.g., for inference endpoints or model downloads) is also possible; the huggingface-hub library is included. However, these remain cloud-based: when using them, the prompts are sent to external services. The design ensures only one provider is used at a time per query – the user selects Local vs Cloud in the UI, or configures a default. For example, if Cloud is selected, all Chat and agent calls will go to the OpenAI/OpenRouter API, whereas if Local is selected, they go to Ollama’s API. This “one or the other” approach avoids mixing sources in a single conversation, which could complicate consistency.
The model management also involves model settings: the user can adjust parameters like temperature, max tokens, etc. either globally or per request. Those are respected by the backend when constructing API calls to Ollama or OpenAI (as seen with the temperature field in chat requests).
The system can be extended with additional providers if needed (for instance, hooking up to a local HuggingFace Transformers pipeline if someone has models in memory; the libraries are present, including PyTorch and Transformers). But by default, local = Ollama, cloud = OpenAI family.
Retrieval-Augmented Generation (RAG) Architecture: A core feature is the ability to ground AI responses in user-provided data (documents). The RAG implementation consists of: - A Vector Store using FAISS (Facebook AI Similarity Search) to index document embeddings. When the user adds documents, a separate indexing process converts these documents into numerical vectors and stores them in an index file on disk (index.faiss) along with metadata (meta.jsonl for text and file references). - Document Ingestion: Supported document formats include PDF, Word (.docx), text (.txt, .md) as indicated by the file scanner. Ingestion is typically done by running the script rag\_index.py or via a background job. This process likely uses SentenceTransformers (a pre-trained embedding model, e.g., all-MiniLM-L6-v2) to embed text chunks. PDF parsing is done with pypdf, Word with python-docx, etc., to extract text. The text is chunked (possibly by paragraphs or a fixed token length) and each chunk’s embedding is computed. The resulting vectors are inserted into the FAISS index along with metadata (document path, chunk text snippet, etc.). If a GPU is available, faiss-gpu can be used for faster indexing/search; otherwise faiss-cpu is used. - Embedding Strategy: The system supports both offline and online embedding. Offline mode uses a deterministic hashing trick for testing (as seen in the code, if no model is available, it creates a pseudo-embedding as a one-hot vector based on a hash of the text). Online mode uses a real embedding model. If EMBED\_MODEL\_PATH is configured to a local model path, it loads that with SentenceTransformers. If not, but online\_mode is True, it will download the model by name (like all-MiniLM-L6-v2 from HF Hub) and use it. This flexibility ensures the RAG can run even without internet or heavy models (though with degraded quality in offline dummy mode). - Querying: When a user or an agent issues a query, the system embeds the query text using the same embedding model (or method), then performs a similarity search in the FAISS index for the top k most relevant chunks. The search results come with similarity scores and the stored snippet text. The backend returns these results (with file path references and snippet content) to the caller (either the UI or an agent). To improve determinism in tests, there’s a minor reranking that counts keyword overlap if in test mode, but generally results are by cosine similarity. - Using Retrieved Data: If the query was from the RAG UI page, the front-end shows these snippets and highlights them. If the query is part of an agent’s task (e.g., an agent trying to answer a question), the agent will incorporate the snippet text into its context or prompt. In a typical RAG pipeline, the retrieved texts are appended as context with citations when asking the LLM to produce a final answer. The system’s RAGSearchTool likely does exactly this: fetch the docs and then form an augmented prompt for the LLM. - Data Flow Summary: The flow is Document -> Embedding -> Index -> Query -> Retrieve -> Answer. For example, if a user asks “What does the quarterly report say about revenue?”, the system will: 1. Take the question, embed it into a vector. 2. Search FAISS index to get, say, 3 most similar passages (likely from the “Q3\_report.pdf” that mention revenue). 3. Return those passages. 4. The LLM (through an agent or directly via a special prompt) is then given the question plus those passages and asked to formulate an answer using them. The answer will include references (like “[Source: Q3\_report.pdf]”) which the UI links to the document viewer.
This approach ensures the AI’s answers are grounded in the actual content the user provided, and thus verifiable.
Database Layer: All persistent operational data is stored in a SQLite database (default file name might be unified\_system.db in the project root). SQLite is used for simplicity and local deployment benefits (no external DB required, and it’s efficient for single-user scenarios). The database is accessed via SQLAlchemy, providing an ORM for ease of development. There might be two layers: a synchronous core (for threads) and an async layer (for async queries via aiosqlite), as hinted by naming in architecture doc.
The schema includes several tables: - executions: main table for task/workflow execution records (execution ID, task\_id or workflow\_id, start\_time, end\_time, status, etc.). - execution\_steps: detailed steps for each execution (linked by execution\_id). Each step could log the action an agent took (tool used or message content) and the outcome. - execution\_logs: possibly a more granular or textual log of everything that happened during execution (or might be merged with steps). - artifacts: references to output artifacts from executions (files generated, data saved), with fields like execution\_id, file path, type of artifact. - schedules: stores schedule entries (cron expression, task\_id, next\_run, enabled flag, etc.). - event\_subscriptions: stores event triggers (event type, filter criteria blob, target task/workflow). - workflows: stores workflow definitions (probably as a serialized JSON of nodes or a reference to a YAML). - workflow\_nodes: if workflows are complex, nodes/edges could be stored in a related table (each node references a task and has relationships to next nodes). - metrics: might store periodic snapshots of system metrics (timestamp, CPU, memory, etc.) if the system logs them for historical trends. - alerts: records of any system alerts or notifications (e.g., if a task fails or resource threshold exceeded).
There are likely additional tables for config or user accounts (if authentication is expanded), but from what's given, the above covers the main automation data.
The SQLite DB is set to WAL mode which improves write concurrency and performance for read-heavy usage. The system also creates several indexes to speed up queries (for example, an index on execution status or timestamps for sorting history). In local use with modest data volumes, this is sufficient. If scaling up (multi-user or very high load), the recommendation is to move to PostgreSQL, but that’s optional.
The ORM models (using SQLAlchemy or Pydantic’s ORM if any) enforce constraints and allow easy query building. For instance, when the UI calls GET /api/v1/executions, the backend will query the Execution model and join with Steps or filter by status, etc., then serialize to Pydantic models for the API response.
Overall, the AI & Data layer ensures that knowledge is captured (via documents), stored efficiently (via embeddings and database), and utilized by the AI in a controlled manner. It bridges the gap between stateless AI model calls and stateful data relevant to the user’s needs.
System Dependencies & Integrations
The project relies on a number of Python packages and external services. Below is an explicit list of all major dependencies (as specified in the requirements) and their roles in the system:
Core AI & Agent Frameworks:
SmolAgents (smolagents[openai]>=0.4.0): The primary agent orchestration library used to define and run AI agents with tool-using capabilities. It provides a high-level API for prompting LLMs and parsing their responses for tool invocations, and is tightly integrated with OpenAI-compatible LLM APIs.
Transformers (transformers==4.56.2) and Tokenizers: Hugging Face Transformers library and its fast tokenizer backend. These are likely used for any local embedding or model operations outside Ollama, and to ensure compatibility with model prompting (e.g., counting tokens or using HF models for embedding).
SentenceTransformers (sentence-transformers==5.1.1): Used to generate text embeddings for RAG. It downloads/loads embedding models (like MiniLM) to compute document and query vectors.
huggingface-hub (==0.35.0) and safetensors: Allow downloading models and datasets from Hugging Face, and safetensors is a secure model file format. These support SentenceTransformers and any direct HF model usage.
OpenAI Python SDK (openai>=1.0.0): Allows direct calls to OpenAI’s API if configured (for chat completions, etc.). SmolAgents might use this under the hood if a task is configured to use an OpenAI model.
PyTorch (torch, torchaudio, torchvision): Provides the deep learning backbone for local model execution and embedding computations. For example, SentenceTransformers runs on PyTorch. Torch is installed without a version pin because it will pick the appropriate build (CPU or CUDA) for the environment.
Multi-Agent & AI Governance:
pyautogen (>=0.2.0): Microsoft’s Autogen framework for multi-agent dialogues. Facilitates creating multiple chatbot agents that can converse. Used in the Multi-Agent subsystem.
agent-framework: Possibly another component of Microsoft’s Autogen (or a similarly named library) for building agent behaviors.
LangChain (>=0.3.0) and related packages (core and community): A toolkit for constructing LLM-powered applications. Provides classes for Chains, Agents, memory, and integration with various vector stores and APIs. Even if SmolAgents is primary, LangChain may be used for certain chains or memory management.
LangGraph (>=0.6.0): A library for representing conversation and reasoning flows as graphs. It likely integrates with LangChain or standalone to implement the AI Governance Engine’s structured plans. It can enforce an LLM to follow a predetermined graph of steps (ensuring no deviation).
jsonschema (>=4.20.0): Used to validate structured outputs from LLMs against a schema. For example, if the AI is asked to produce a JSON for a task plan, jsonschema can verify it matches the expected format, catching errors in AI output.
Retrieval and Data Handling:
FAISS (faiss-cpu>=1.9.0): The vector similarity library used to index and search embeddings. Enables fast KNN searches in high-dimensional space for the RAG feature. (Note: if GPU is present, a faiss-gpu could be used manually, but pip default is CPU).
PyPDF (pypdf==6.1.0): For reading PDF files and extracting text to index.
numpy (>=2.3) and pandas (==2.3.3): NumPy is used throughout for numerical computations (embedding arrays, etc.). Pandas might be used if tasks involve data frames or for any data analysis in tasks.
Pillow (>=10.4): The PIL library for image processing. Possibly used if any image data is embedded or if the UI or tasks handle images (for example, maybe converting images in documents to text via OCR, or generating image outputs).
scikit-learn (>=1.5): Might be used for utility functions (FAISS sometimes pairs with sklearn for clustering or dimensionality reduction) or for any analytical tasks the user creates. It’s included likely to support data science use-cases.
Office and Utility Tools:
python-docx (==1.2.0): To read/write Microsoft Word documents. Used by the WordReportTool to create .docx files from agent outputs, or to parse .docx for RAG.
openpyxl (==3.1.5): To read/write Excel spreadsheets. Enables agents to create or modify .xlsx files (for reports, data logging, etc.).
ddgs (>=2.0.0): Stands for DuckDuckGo Search. A library that provides a simple interface to DuckDuckGo’s search results (probably bypassing the need for an official API). The DuckDuckGoSearchTool uses this to let an agent search the web.
pyperclip (>=1.8.2): A cross-platform clipboard utility. Possibly used by the system to copy content to clipboard (maybe a feature like “copy code to clipboard” uses it on the server side, or an agent could use it to put text into the clipboard as an action).
Windows-Specific (optional):
pywin32 (>=306) [Windows only]: Provides access to Windows COM objects and APIs. This is what enabled Outlook desktop integration via COM. If the user is on Windows and wants to use Outlook directly, installing pywin32 allows the backend to control Outlook (for reading and drafting emails). However, in the new plan using IMAP, this would be optional.
msal (>=1.30.0): Microsoft Authentication Library. Used for the Microsoft Graph API integration (to authenticate and access Office 365 data). If a user wanted to connect to Outlook 365 via Graph (instead of IMAP), this library handles the OAuth flows and token management. Again, with IMAP, this becomes optional and likely not used unless explicitly configured.
System & Automation:
psutil (>=5.9.0): Used to gather system metrics (CPU load, memory, disk, process info). The health checker uses this for system info.
APScheduler (>=3.10.4): Advanced Python Scheduler for scheduling jobs (cron/interval) within the app. All scheduling of tasks uses this to manage timing.
GPUtil (>=1.4.0): Utility to get GPU stats (NVIDIA GPUs). Helps in monitoring GPU usage and deciding on which device to run models (the get\_optimal\_device logic may use GPUtil to pick a free GPU).
watchdog (no version specified, but included): Observes filesystem events. Powers the file sensor for event triggers on file changes.
sqlalchemy (version unspecified in snippet, but included): The ORM for database access. Provides models and query capabilities for all persistent data.
Config & Utilities:
python-dotenv (==1.0.1): Loads environment variables from a .env file. Simplifies configuration by allowing secrets (API keys, etc.) to be placed in .env and automatically loaded at startup.
PyYAML (==6.0.2): Parses YAML files. Used to read config.yaml and any YAML-based task or workflow definitions.
Pydantic (>=2.6,<3): Data validation and modeling library. FastAPI v0.109 uses Pydantic v2 for defining request and response models (e.g., HealthResponse, Task schemas). Pydantic ensures incoming data meets types and provides .model\_dump() for serialization.
typing-extensions (>=4.9): Provides backports of newer typing features for compatibility (useful for Pydantic and other libraries in Python versions before certain typing improvements).
Additionally, matplotlib (>=3.9) is listed. This could be used for generating charts or plots (perhaps for the Learning Insights or any analysis tasks). It's not directly used in UI (the UI uses Plotly), but an agent could use matplotlib to produce a graph image as an output, for instance.
API & Server:
FastAPI (>=0.109.0): The web framework for the REST API. It leverages Starlette under the hood for the ASGI server and defines the routing, dependency injection, and docs.
Uvicorn ([standard]>=0.27.0): The ASGI server used to run FastAPI. Standard extras include uvloop and websockets support. We run uvicorn main:app or similar to start the backend.
Requests (>=2.32): A popular HTTP client. Likely used for any synchronous HTTP calls (maybe some legacy code or within tasks to call external URLs). However, the asynchronous HTTPX is used within the chat route and possibly others for async calls. Requests might be used by certain tools or older code segments where blocking HTTP is fine.
Frontend & Testing:
Reflex (>=0.5.0): The UI framework library. This includes the server and client code for the Reflex app, enabling component definitions and state management.
httpx (>=0.27.0): As noted, an async HTTP client used by Reflex backend to call FastAPI.
websockets (>=12.0): The WebSocket protocol implementation, likely used by Reflex and possibly by FastAPI if any direct websocket routes were added.
plotly (>=5.18.0): Used in the Reflex UI to create charts for monitoring, etc..
python-multipart (>=0.0.9): Required by FastAPI for handling file uploads (Used in RAG upload endpoint to parse multipart form data).
aiofiles (>=23.0.0): Used by FastAPI for async file I/O (and by Reflex to save uploads).
pytest (>=8.0.0) and pytest-asyncio (>=0.23.0): Testing frameworks to write unit and integration tests, including async tests for async endpoints or state changes. They ensure the system functions as expected and help catch regressions.
All these dependencies work in concert. For instance, when an agent wants to use a tool that requires internet, requests or httpx might be used. When the system is packaging text to embed, it relies on transformers and torch. The UI’s reflex app uses httpx to call the FastAPI which uses Pydantic to validate input and might call openai or ollama depending on config.
Interactions between Dependencies: - SmolAgents and OpenAI/HF – SmolAgents abstracts the LLM calls but under the hood uses the OpenAI SDK (if model is cloud) or an OpenAI-compatible local endpoint (Ollama which implements the same API schema). So SmolAgents will send prompts via openai library or via httpx to Ollama’s API depending on configuration. - Autogen (pyautogen) – builds on top of the LLM calls too, likely using OpenAI or other models. It might use the agent-framework library to manage multiple agents; this in turn uses asyncio to have agents "talk" in parallel. It will definitely use the same OpenAI or local model interfaces. - LangChain – could integrate with FAISS (LangChain has a FAISS wrapper) and with OpenAI. It might also be used for prompt templates or chain logic in the Task Generation endpoints (to format the user’s request and parse the response). - LangGraph – sits between LangChain/SmolAgent and the actual LLM to enforce structure. For example, if a plan is required in JSON, LangGraph could generate a graph that first asks the LLM to output JSON, then validates it (via jsonschema), then proceeds. - Database and Async – SQLAlchemy is used in a multi-thread or multi-async environment. Likely they configure it with an async engine for use with FastAPI endpoints (through core.database\_async) and possibly a synchronous engine for any threaded jobs. SQLite’s WAL mode and thread-local connections help avoid cross-thread conflicts. - APScheduler and asyncio – APScheduler jobs that need to interact with async code (like enqueuing a task in FastAPI) must be careful. Possibly they use a ThreadPool for APScheduler, which then calls into the FastAPI context. Or use AsyncIOScheduler if running in the event loop. The integration ensures scheduled jobs can call FastAPI functions or directly database sessions properly. - Reflex and FastAPI – They are separate servers but integrated via HTTP calls. The Reflex backend might also open WebSocket connections to FastAPI if needed (though not likely; usually Reflex only holds websockets with the client). Instead, for pushing events from FastAPI to UI, one strategy could be: FastAPI, when a task completes, could send a POST to a Reflex endpoint to update state. However, more likely the Reflex app includes a background task that polls certain FastAPI endpoints (like health or execution status) at intervals and updates state. The dependencies httpx and asyncio allow that. - Watchdog (file events) and Event Bus – The file events come in on a separate thread (watchdog observer). The integration uses thread-safe queues or direct calls to FastAPI’s event loop via asyncio.run\_coroutine\_threadsafe to emit events in the main loop. This ensures thread boundaries are handled. - Email (IMAP) and AI – The IMAP integration likely uses standard libraries (Python’s imaplib or an external one) to fetch emails. That is synchronous I/O. Possibly they run it in a threadpool or use an async IMAP client if available. When an AI agent calls an email tool, it invokes those libraries, gets data, then uses openai or local LLM to draft a response. The MSAL library would be used if user chooses to connect via OAuth (less likely now).
Environment Configuration: The system uses two main configuration approaches: environment variables and a YAML config file. - Environment Variables (.env): These hold sensitive information and machine-specific settings. For example: - API\_AUTH\_TOKEN – if set, the FastAPI will require this token for any API call (used by verify\_token). - OLLAMA\_BASE\_URL – used to point to the Ollama server URL, especially needed if running in a VM/WSL scenario. By default it might be http://localhost:11434 but can be changed. - OPENAI\_API\_KEY or OPENROUTER\_API\_KEY – keys for cloud model APIs, to be used by the OpenAI SDK or passed in headers for OpenRouter. - HUGGINGFACEHUB\_API\_TOKEN – if using HuggingFace Hub for any model downloads that require auth. - EMAIL\_USERNAME, EMAIL\_PASSWORD, EMAIL\_IMAP\_SERVER, EMAIL\_SMTP\_SERVER – details for connecting to email via IMAP/SMTP, if not stored in config.yaml. - LOG\_LEVEL – to control logging verbosity globally (e.g., DEBUG vs INFO). - These are loaded at startup via python-dotenv, and also accessible in Reflex (Reflex can pass environment variables to the frontend build for any public config like API base URL if needed).
Config File (config.yaml): A YAML file in the project that defines various settings in a structured way. Key sections include:
model: which model provider to use, default model name, temperature, etc. E.g., provider: "ollama" or "openai", model: "qwen2.5:7b", temperature: 0.2.
agent: settings for the AI agents, such as which tools are enabled, maximum tokens to use, or default agent type. Could include toggles for safe mode or output format.
automation: settings for scheduler and events, like max\_workers (how many tasks can run concurrently), default time zone for schedules, or enabling/disabling certain sensors.
api: server settings like host, port (though those might be separate as FastAPI parameters), allowed CORS origins (which by default includes localhost and Reflex ports), and whether auth is enabled.
performance: toggles for caching (enable query cache, size=1000, TTL=5min as in the architecture), and any model reuse settings (like keeping a pool of loaded models in memory).
multi\_agent: whether to enable the multi-agent feature (since running multiple agents can be heavy, maybe default false). Could also define roles or agent counts (like number of critic agents).
email: (if present) to configure IMAP/SMTP details in a user-friendly way rather than requiring environment vars.
The config.yaml is read at startup (in load\_api\_config() as seen in code). If it’s missing, defaults are used for critical values (like default port 8502). The application likely passes relevant config to components: e.g., the number of max\_workers from config might be used to initialize the ThreadPool for tasks.
Secrets Handling: All API keys and passwords are stored in the environment or .env file, which is not checked into source control. The code never hardcodes these values. For example, when needing OpenAI key, it reads os.getenv("OPENAI\_API\_KEY"). For IMAP, reading EMAIL\_PASSWORD. This way, secrets can be managed securely (the user or deployer sets them in their environment). The documentation likely instructs the user to create a .env with the needed keys. This approach prevents accidental leakage of sensitive info in logs or repository.
In summary, the system depends on a stack of well-known libraries. Each is chosen for a specific reason (FastAPI for fast web APIs, Reflex for modern UI, SmolAgents for simpler agent logic than raw LangChain, etc.), and the integration between them is carefully managed. Proper configuration and environment setup is key for these to work together (e.g., pointing the Reflex app to the correct API URL and ensuring the same auth token is used by the UI). The blueprint provided by this document, along with requirements and config templates, allows a development team to set up all these components in harmony.
Automation & Monitoring Layer
Automation and monitoring are critical capabilities of the Local AI Agent system, enabling it to function autonomously and remain reliable over long periods. This layer encompasses the scheduler, event system, logging, and health monitoring/alerting features.
Scheduler Structure (APScheduler Jobs): The scheduler is the component that handles timed execution of tasks. It is built on APScheduler and is configured likely as a BackgroundScheduler started with the FastAPI app (maybe in the startup event). The scheduler pulls scheduled job definitions from the database (schedules table) and adds them as jobs. Each job has: - A trigger (CronTrigger or IntervalTrigger based on the schedule pattern the user set), - A job function (which typically enqueues the specified task for execution), - An identifier (so it can be modified or removed if the user updates the schedule).
For example, if a user schedules “DailyReportTask” at 8:00 AM every weekday, an APScheduler CronTrigger for 0 8 * * MON-FRI is created, and at those times, APScheduler will call something like automation.engine.enqueue\_task("DailyReportTask"). The system likely wraps this call in a try/except to log scheduling failures (if any).
APScheduler runs jobs in the background. Because our tasks themselves run in the main execution engine (potentially asynchronously), the scheduler’s job might simply submit an async task to the FastAPI loop. One strategy is to use APScheduler with a ThreadPoolExecutor for job execution – then the job function uses asyncio.run or an endpoint call to start the actual task. Another is using AsyncIOScheduler (since FastAPI runs under uvicorn’s event loop) to schedule coroutines directly.
Beyond user-defined schedules, APScheduler is also used for system maintenance jobs: - System Health Check Task: Although a health endpoint exists, the system might periodically compute certain metrics or check external services. For instance, a job every 5 minutes could ping the Ollama server or check GPU memory and log if something is off. Or update a metrics table so historical data is collected rather than only on demand. - Data Backup Task: Optionally, a nightly backup job could dump the SQLite DB and key files to a timestamped copy (especially before performing an update or cleanup). - Cleanup Task: A job that runs, say, weekly to prune old execution logs beyond a retention period (the logs table or artifacts older than X days to prevent the database from growing indefinitely). - Email Polling Task: If IMAP needs polling, a scheduled job every few minutes could check for new mails (unless an IDLE push approach is used).
APScheduler also integrates logging; if a job errors out (exception not caught), APScheduler can log it. The system likely configures a listener to record job failures, which could then raise an alert or appear in the monitoring UI.
GPU/CPU Monitoring & Alerts: Using psutil and GPUtil, the system monitors resource usage. This can be done on-demand (when UI queries /health) and/or continuously. In a continuous setup: - A background thread or async task can sample CPU and memory every second, updating an in-memory structure. The UI’s health bar might subscribe to these updates over websocket for real-time feedback (Reflex can push state changes quickly). - GPU stats (memory usage, load) can be polled similarly. If a GPU is present and running models, its usage might spike during model inference; the monitoring tries to capture that. - If any metric exceeds a threshold (like CPU > 90% for >10 seconds, or memory near 100%, or GPU memory full, etc.), the system could generate an alert. An alert might be logged in the alerts table or trigger a notification in the UI (e.g., showing a red icon or a toast message “High Memory Usage Detected”). - Temperature could also be monitored (GPUs often report core temp; if it’s dangerously high, an alert could advise the user). - The monitoring page displays these metrics in charts. Plotly graphs could show history of last N minutes. This likely relies on having some recorded time series. If not stored in DB, it might be kept in a ring buffer in memory.
Logging: Comprehensive logging is in place for both audit and debugging: - The application likely uses Python’s logging module configured with rotating file handlers. According to architecture, logs are kept in logs/agent\_system.log (for general info/debug) and logs/agent\_system\_errors.log (for errors). They rotate daily and keep about 30 days of logs by default. - Every major action is logged: incoming API requests (via logging middleware), task start/stop, agent tool usage, errors/exceptions, and system events (e.g., “Scheduled task X executed”, “File created event received”). - The log format typically includes timestamp, severity level, source module, and message. For example: 2025-10-23 10:00:00 INFO automation.engine: Task "DailyReportTask" enqueued by scheduler. - Error logs will include stack traces and the context. If an exception occurs in an agent’s code execution, it’s caught and logged with details. - The UI’s Logs page reads from these files to let users see what's happening behind the scenes. The file read might be done with aiofiles to not block the server, reading the last few hundred lines for display. - Structured logging: Possibly logs are also output in JSON for easier ingestion by tools, but given local scope, simple plain text is fine.
Watchdog & Event Triggers: We discussed the FileSensor that uses watchdog to emit events on file changes. This is part of a broader concept of sensors: - A sensor is a component that watches some external source and translates it to internal events. The system has at least: - FileSensor (for filesystem events), - (Potentially) EmailSensor (if not using schedule polling, one could imagine using IMAP IDLE via a library or Graph webhook to get real-time email arrival, then emit event). - Webhook Listener: The /api/v1/webhooks endpoints can act as a sensor for external HTTP calls. For example, a user might set up an external service (like GitHub) to send a webhook to /api/v1/webhooks/github on a new issue; the backend receives it and emits an event EventTypes.WEBHOOK\_RECEIVED with payload. The event system then triggers any task subscribed to that webhook. - Timer Sensor: Not needed because APScheduler covers timers. - The event definitions (EventTypes) likely include things like FILE\_CREATED, FILE\_MODIFIED, FILE\_DELETED, FILE\_MOVED (we saw those), EMAIL\_RECEIVED, WEBHOOK\_RECEIVED, perhaps SYSTEM\_START (trigger tasks on startup), etc. - The Event Bus (automation.events) manages subscription matching. It could be as simple as checking event type and maybe doing string containment for file paths, or it might allow regex patterns. For emails, a filter might be folder or sender address. - This decoupling (sensors emitting events, and the bus handling them) means the system is extensible: one can add new sensors for different sources (IoT sensors, chat messages, etc.) without changing how tasks are triggered.
Backup Mechanisms: Safeguarding user data (especially their accumulated documents, task scripts, and the execution database) is important: - The scripts/backup.sh likely zips up the unified\_system.db, the tasks/ directory, the docs/ (if not too large), config.yaml, and possibly logs/ into a timestamped archive. The user or a scheduled job can run this. scripts/restore.sh would revert such an archive. - Within the Reflex UI, the Config Backup Service (noted in search results) suggests the UI may allow the user to download a backup of their configuration or entire state. Possibly on the Config page, a "Backup Config" button triggers an API that dumps config and environment info into a JSON, saved in reflex\_ui/config\_backups/backup\_.json. This might include user customizations or just UI state, but likely global config. - Additionally, tasks themselves are just Python files on disk (the tasks/ folder). Users are encouraged to use version control or at least backup these files, since they represent their custom logic. The backup script would include them. - If the system performs an upgrade (in future, via an update workflow), it might automatically back up the critical files, then allow the user to restore if something goes wrong (the mention of "Post-Recovery" in the architecture doc suggests a scenario where an update script crashed and a backup was needed).
Concurrent Execution and Resource Limits: In automation, the ability to run multiple tasks concurrently is a feature, but it must be controlled: - By default, the system sets a maximum of perhaps 5 concurrent tasks in normal mode, and higher (like 12) in optimized mode. This is configured in config.yaml under performance/workers. The Execution Engine enforces this by only allowing that many threads or async tasks at once; additional tasks queue up. - The user can change this if their hardware supports more parallelism (e.g., increase to 12 or more if they have many CPU cores). - Some tasks might be CPU-bound (like running an ML model or heavy computation) so too many at once degrade performance. The system might tag tasks with a “heavy” flag and restrict heavy tasks from running simultaneously beyond a certain number. - The agents themselves might also have concurrency internally – for example, multi-agent sessions spawn multiple asynchronous LLM calls concurrently (the researcher and the coder agent thinking in parallel). The system must ensure this doesn’t starve resources. They likely use asyncio tasks for those and gather results, leveraging Python’s ability to have many in-flight IO operations (like multiple OpenAI API calls at once if needed).
Monitoring Dashboard Integration: The Reflex UI’s monitoring page ties into this layer by: - Periodically calling the health API to get an overview (which is instantaneous values). - Possibly calling a metrics endpoint that streams or provides historical data (if implemented). - Using websockets: If the Reflex state subscribes to a state var like AppState.cpu\_usage, and the backend updates that every second, the UI will reflect real-time changes in a progress bar or chart. - Plotly charts for historical data: The backend could send an array of values (e.g., CPU usage over last 60 seconds) on each update, or the frontend could accumulate data by appending the latest value each second. - The GPU panel specifically might show GPU memory and compute usage, updated every few seconds. This uses GPUtil under the hood.
Alerting and Fail-safes: Automation means the system might run unattended, so it’s important to alert the user to issues: - If a scheduled task fails (throws an exception or times out), the system could mark it and perhaps highlight in the UI (like an error icon next to the schedule or an entry in an Alerts list). The user can then investigate via the Logs or Executions page. - If an event trigger fails to execute its task (maybe the task wasn’t found or had an error), similarly log and alert. - System alerts (like inability to reach an external API): e.g., if OpenAI key is invalid or network is down, the health check would include that and the UI can show a warning (like “Cloud model API unreachable”). - Watchdog on the application itself: perhaps a simple watchdog thread monitors that main components are responsive. For instance, if the event loop is blocked for too long (which shouldn’t happen with async design), or if memory usage gets critically high (risking OS OOM), it could preemptively stop scheduling new tasks and notify the user. - The design likely anticipates safe failure: if an agent tries something dangerous or not allowed, the governance layer or sandboxing will stop it rather than crashing the whole app. Tools like file write might restrict access to only certain directories (ensuring an agent doesn’t overwrite system files). The system could enforce that via configuration (like tasks run under a restricted user account or inside a container, though not explicitly stated, it’s a consideration).
In conclusion, the Automation & Monitoring layer ensures that the Local AI Agent operates reliably, safely, and transparently. Scheduling and events let it run tasks without direct user prompts, enabling automation. Meanwhile, comprehensive monitoring, logging, and alerting give the user and developers insight into what the system is doing and whether anything needs attention. This careful balance allows confidence in leaving the AI agent running continuously as a personal assistant/service on one’s machine.
Development & Deployment Instructions
This section provides a blueprint for developers to set up the project environment, understand the project structure, and deploy the application in both local (development) and production scenarios. By following these instructions, a development team can reconstruct the system and ensure it runs as expected.
Directory Structure & Project Organization: The repository is organized into directories separating different concerns. A simplified top-level structure is as follows:
local-agent/
├── api/ # FastAPI backend application
│ ├── app.py # FastAPI app initialization and router inclusion
│ ├── auth.py # Authentication (token verification)
│ ├── models.py # Pydantic models (HealthResponse, ErrorResponse, etc.)
│ ├── middleware/ # Custom middleware (logging, error handling, security, rate limit)
│ ├── routes/ # All API route definitions (tasks.py, schedules.py, events.py, etc.)
│ └── ... (other API related modules)
│
├── automation/ # Automation engine components
│ ├── engine.py # Core execution engine for tasks and workflows
│ ├── events.py # Event bus and event handling logic
│ ├── queue.py # Task queue management (thread pool or async queue)
│ ├── startup.py # Initialization of automation system (maybe starts sensors, scheduler)
│ └── ... (schedulers, helpers)
│
├── tasks/ # User-defined task scripts (Python files for each custom task)
│ ├── task\_example.py # (Example task implementation)
│ └── ... (more tasks added by user)
│
├── workflows/ # (If workflows are defined as separate files or YAMLs)
│ └── example\_workflow.yaml
│
├── sensors/ # Sensors for event detection
│ ├── file\_sensor.py # Watches filesystem and emits events[166]
│ └── ... (potentially email\_sensor or others)
│
├── health/ # System health check modules
│ ├── health\_checker.py # Aggregates health info from various submodules
│ ├── outlook\_health.py # Checks email integration health (Outlook/IMAP status)[167][168]
│ └── ... (maybe hardware\_health.py, etc.)
│
├── ai\_gov/ # AI Governance module (ensuring safe/structured AI operations)
│ ├── orchestrator.py # Orchestrates LangGraph or governance workflows
│ └── ... (rule definitions, etc.)
│
├── utils/ # Utility modules
│ ├── device\_manager.py # Chooses GPU vs CPU for computations (get\_optimal\_device)[169]
│ ├── ollama\_config.py # Utility to determine Ollama base URL (handles WSL case)[170]
│ └── ... (other helpers: file utilities, etc.)
│
├── reflex\_ui/ # Reflex front-end application (self-contained subproject)
│ ├── reflex\_ui.py # Entry point for Reflex app
│ ├── rxconfig.py # Configuration for Reflex (ports, API base URL)[171]
│ ├── components/ # Reusable UI components (sidebar, health bar, etc.)[172]
│ ├── pages/ # Page components for each route (dashboard.py, chat.py, rag.py, etc.)
│ ├── state/ # State management classes (AppState, specific page states)[31]
│ ├── services/ # Backend-call services (e.g., api\_client.py for HTTP calls to FastAPI)[33]
│ └── ui/ # Design system (theme.py, components like button.py, layout, icon definitions)[173][174]
│
├── docs/ # Documentation and guides
│ ├── GETTING\_STARTED.md
│ ├── ARCHITECTURE.md # High-level architecture description[175]
│ ├── OPENROUTER\_SETUP.md
│ └── ... (other help docs or design notes)
│
├── scripts/ # Helper scripts for setup and maintenance
│ ├── start\_all.sh # Shell script to launch both API and UI for convenience[176]
│ ├── run.sh # Script to run AI assistant in CLI profiles (assistant, coder, visual)[177]
│ ├── backup.sh # Backs up data (DB, config, tasks)
│ ├── restore.sh # Restores from backup
│ └── ... (others like install\_dependencies.sh)
│
├── config.yaml # Main configuration file (in YAML format)[162]
├── .env # Environment variables (not committed, user-provided)
├── requirements.txt # Python dependencies (compiled from requirements.in)
├── requirements.in # Human-edited list of dependencies with comments[178]
├── unified\_system.db # SQLite database file (created at runtime)
└── logs/ # Directory for log files
 ├── agent\_system.log
 └── agent\_system\_errors.log
This structure separates front-end and back-end cleanly. The reflex\_ui is essentially a separate project that can be developed in isolation and communicates with the back-end via HTTP. The back-end contains submodules by concern (api, automation, tasks, sensors, etc.), making it easier to maintain.
Environment Setup: 1. Python Installation: Ensure Python 3.11 or 3.12 is installed on the development machine. Python 3.12 is recommended for performance improvements (as noted in the project README). 2. Creating Virtual Environment: Create a virtual environment for the project. For example:
python3.12 -m venv .venv312
source .venv312/bin/activate
This will isolate project dependencies. 3. Install Dependencies: Use pip to install the required packages. You can use pip install -r requirements.txt. If requirements.txt is not present, first compile it from requirements.in (requires pip-tools if one wants to update, but for usage the compiled txt is provided). This will download all needed libraries. - If on Windows and planning to use Outlook COM, also run pip install pywin32 as needed. - If certain heavy libraries have issues (like faiss), refer to any platform-specific notes. For instance, on Windows, faiss-cpu may need Visual Studio Build Tools. On Linux, a wheel should install easily; if not, use conda as comment suggested. 4. Install Ollama (Optional for local LLM): If using local models, install Ollama from its official source (https://ollama.ai). After installing: - Run ollama serve in a separate terminal or as a background process. Ensure it listens on port 11434 (default). - Pull the required models: e.g., ollama pull qwen2.5:7b (for a general model). Also pull others like qwen2.5-coder:14b-q4\_K\_M if coding assistance is needed, and qwen2-vl:7b for vision-language if applicable. Each model may be hundreds of MBs to a few GBs, so ensure enough disk space and time to download. - If running in WSL (Linux on Windows) and Ollama is on Windows side, use the provided instructions to set OLLAMA\_BASE\_URL to the Windows host’s Ollama service. Essentially:
export WINDOWS\_HOST=$(grep nameserver /etc/resolv.conf | awk '{print $2}')
echo "OLLAMA\_BASE\_URL=http://$WINDOWS\_HOST:11434" >> .env
And on Windows, set OLLAMA\_HOST=0.0.0.0:11434 and restart Ollama so it’s accessible outside localhost. 5. Configure Environment Variables: Create a .env file in the project root. Add necessary keys and config, for example:
API\_AUTH\_TOKEN="devtoken123" # Token for API auth (set in UI as well for calls)
OPENAI\_API\_KEY="sk-..." # If using OpenAI
OPENROUTER\_API\_KEY="or-..." # If using OpenRouter for free models[155]
OLLAMA\_BASE\_URL="http://127.0.0.1:11434" # if not default, or WSL config as above
EMAIL\_USERNAME="user@example.com" # Email credentials for IMAP
EMAIL\_PASSWORD="password123"
EMAIL\_IMAP\_SERVER="imap.gmail.com"
EMAIL\_SMTP\_SERVER="smtp.gmail.com"
Also include any other keys (e.g., Huggingface token if needed, MS Graph client ID if using Graph). This .env will be auto-loaded by the app on start.
Populate config.yaml: Open config.yaml and adjust settings if needed. At minimum, ensure:
api.enabled is true and ports are correct (8502 backend, and allowed CORS origins include the Reflex frontend URLs).
model.provider is set to ollama or openai depending on default preference.
openrouter.enabled is true if using OpenRouter (and ensure the API key is in .env).
automation.max\_workers is set (maybe 5 or 10 by default) to control concurrency.
multi\_agent.enabled to false if you don’t want to load autogen every time, or true to allow that feature.
performance.cache\_enabled etc., if any caches to enable.
Check email section: if an IMAP integration requires specifying folder names or intervals.
Save the config.
Build & Run Instructions (Development): - Starting the Backend (FastAPI): You can run the API with Uvicorn. From the project root (with venv activated):
uvicorn api.app:app --reload --host 127.0.0.1 --port 8502
The --reload flag restarts the server on code changes (good for development). Ensure the port matches what Reflex expects (8502 as configured). If API\_AUTH\_TOKEN is set, all calls must include that token (Reflex will handle if configured). Alternatively, run the provided script:
python start\_api.py
if such a script exists (the README snippet suggests using python start\_api.py for the API). - Starting the Frontend (Reflex): In another terminal (with venv active), go to the reflex\_ui directory:
cd reflex\_ui
reflex run
This will build and launch the Reflex app. On first run, it will install front-end dependencies and start two servers: one on port 3000 for the web UI, and one on port 8000 for Reflex’s backend logic. If port 3000 is in use or you want a different one, adjust in rxconfig.py or run reflex run --port 3001 (and update CORS origins in config.yaml to allow that). After a short build, you should see output indicating the app is running. Open a browser to http://localhost:3000 to view the UI. - Running Both (All-in-One): For convenience, the project may include ./start\_all.sh which probably calls both the API and UI (and possibly ollama serve) in one go. This might open a Streamlit UI if it's an older script, but in context of Reflex v2, developers can adapt it to run uvicorn and reflex concurrently (maybe via npm concurrently or two background tasks in a shell script). In development, it's fine to run them in separate terminals as described.
Initial Setup (First Run): On first launch:
The database unified\_system.db will be created (if not exists) with the required tables. If any alembic or migrations are included, run them (not mentioned, likely simple enough to not need migration tool).
If using RAG, populate the docs/ folder with some test documents and run the indexer:
python rag\_index.py
This will create index.faiss and related files in out/rag or similar directory.
In the UI, navigate to Config or Models page (if exists) to verify the model settings. The UI might allow you to test connectivity to Ollama or OpenAI.
Navigate to Monitoring to see if health stats appear (meaning /health API is reachable and providing data).
Try the Chat page with a simple prompt (“Hello”) to ensure the model responds. If not, check the console for any errors (e.g., Ollama not running or API key issue).
Create a simple task via the Tasks page (AI Task Creator): e.g., description “Say hello by printing a message”. The AI should generate a task file in tasks/ (check the directory). Then run it to ensure the execution flow works.
Local vs Production Deployment: - In a local/dev scenario, you run with --reload and possibly use SQLite in file mode. For production, you’d want stability, so: - Run Uvicorn without reload (maybe as a system service or using Hypercorn/Gunicorn for multiple worker processes if needed, though with SQLite, multiple workers should be careful due to single-writer limitation). - Use environment variable LOG\_LEVEL=INFO or WARNING in production to reduce log noise. - Possibly disable the FastAPI docs on production (set docs\_url=None in app or behind auth). - Ensure API\_AUTH\_TOKEN is set to a strong secret if the machine is accessible on a network. - Build the Reflex UI for production:
reflex deploy --no-open
Reflex can compile the app into an optimized production build. Actually, Reflex's model is to generate a static bundle and host it. However, since our Reflex is dynamic (depending on FastAPI), we still need the Reflex backend running. But we can run reflex run --env prod to minimize debug info. Reflex in prod mode also uses compiled assets, ensuring faster load. - Packaging to .exe: To distribute to end-users (who might not have Python), you can create an executable: - Using PyInstaller: Create a spec file or use pyinstaller to package the API and Reflex. This is tricky because we have two processes (API and UI). One approach: you package the API as a console exe and also package Reflex as another (or provide the static frontend separately). A simpler way is to provide a one-click script that sets everything up, but as per requirement, an .exe is needed. - Alternatively, use Nuitka or cx\_Freeze which might handle multiple entry points. - For instance, a packaged app could, on launch, start the FastAPI (embedded) and start a browser pointing to the Reflex frontend. - Another angle: since Reflex can compile to a static web app, you could host the static files with the FastAPI itself (using Starlette StaticFiles) and avoid needing two servers. That might complicate Reflex’s server state usage though. If feasible, this consolidation would ease creating one executable. - In any case, building an .exe will require bundling the Python interpreter, all libraries, and possibly the model files if you want to include a default model (though probably too large; better to ask user to download model separately). - This process is environment-specific, so for Windows distribution, use PyInstaller on Windows to create the .exe. Test the .exe extensively (ensuring all relative paths like config.yaml, models, etc., are correctly referenced – you may need to set those up in a known location or package them as data files). - The output can then be delivered to end users who can run it without manual setup. Provide scripts or instructions for installing Ollama and models, since that may remain external.
Docker Deployment (alternative production): The architecture doc suggests a Docker Compose setup:
An API container (Python base image) running uvicorn api.app:app.
A UI container (node or python) running reflex run or serving the built static files (Reflex could even export to Next.js static build).
Nginx as a reverse proxy to unify them on one port, handle HTTPS, and do additional rate limiting or basic auth.
If one doesn’t want separate containers, it’s also possible to run it all in one container (especially if UI is static). But splitting gives separation of concerns.
This approach is suitable for a scenario where multiple users access the system on a server. However, our main target is local usage, so Docker is optional.
Testing & QA: - Running Tests: Use pytest to run the test suite. For asynchronous tests (e.g., calling async API endpoints or simulating multiple tasks), pytest-asyncio is used to await coroutines. For example:
pytest -s tests/
This will run all tests, showing print/log output. Ensure the app is not already running, or tests start their own instance (some tests might spin up a TestClient from FastAPI to simulate calls). - Writing New Tests: For any new feature, create a test module in tests/ (or testing/). Use FastAPI’s TestClient to call API routes and assert responses. Use Reflex’s testing utilities or simply verify the state changes for UI logic (Reflex might not have an easy headless test mode, but one can test the pure Python state functions). - Manual QA: Simulate typical user workflows: - Add a document, index it, ask a question, verify answer and citation. - Create a schedule, wait for it to trigger, verify the task ran at correct time. - Cause an error (e.g., write a task with a bug), run it, ensure it doesn’t crash the whole system and the error is reported in UI. - Try multi-agent with a sample problem and see that agents converse and produce an output. - Test email integration with a test email account (perhaps set up a Gmail with IMAP). Fetch folders and draft a reply to a known email. Check that the draft appears in the email account’s Drafts. - Check that stopping and restarting the system preserves data (tasks, schedules, etc. should persist via the DB). Possibly also simulate a crash or kill to ensure durability.
Throughout development, maintain the principle of modularity and separation. The instructions here ensure each part can be developed and debugged in isolation (e.g., you can run just the FastAPI with a dummy client to test tasks logic, or run just Reflex with a mock API for UI design). This decoupling is beneficial when multiple developers work on front-end vs back-end.
Finally, when the system is ready for deployment, package it according to the chosen method (executable or Docker). Provide documentation for end-users (in an appendix or README) on how to install any prerequisites (like Ollama, models, and if needed, Visual C++ runtime for Python on Windows, etc.).
Functional Breakdown & Interactions
In this section, we break down each major feature and describe step-by-step how the user’s actions propagate through the system, how different components interact, and what the expected outcomes are. We also illustrate typical workflows with concrete examples, tying together the front-end, back-end, AI model, and automation engine.
1. Chatting with the AI (Direct Q&A): - Purpose: Allow the user to ask questions or have conversations with an AI assistant, either for general queries or for coding help, etc. - User Action: The user navigates to the Chat page in the UI, selects a model (Local or Cloud) and enters a prompt (e.g., “Explain the theory of relativity in simple terms.”). - UI -> Backend: When the user submits the prompt, the Reflex UI calls the POST /api/v1/chat/message endpoint with the prompt text, selected model, and temperature. This HTTP request includes the API auth token for verification. - Backend Processing: FastAPI receives the request and passes it to the send\_message function in chat.py. Here’s what happens: - It constructs a JSON payload for the LLM API. If the model provider is local (Ollama), it sets up a call to OLLAMA\_BASE\_URL/api/generate with the model name and prompt. If it were cloud (OpenAI), in a different config, it might call OpenAI’s REST endpoint via the openai SDK (not shown in the snippet, but presumably configured elsewhere). - The call is made using httpx.AsyncClient, awaiting the response. The stream: False parameter means we wait for the full answer. - Once the LLM responds with generated text, the backend wraps it in a ChatMessageResponse Pydantic model along with metadata like timestamp and model name. It also appends the Q&A to an in-memory history list. - Backend -> UI Response: The FastAPI returns the JSON response with the AI’s answer text. The UI receives this via the api\_client service call, and updates the state to display the assistant’s answer in the chat window. - UI Post-Processing: The chat UI might stream the answer gradually. In this implementation, since stream: False, the full answer arrives at once. If streaming was enabled, the back-end could send partial data (but that requires a different mechanism, possibly server-sent events or websockets). - Result: The user sees the AI’s response appear. They can then type another message to continue the conversation. Each interaction cycles through the same steps. If using context, the UI might include recent conversation from history in each new request, or the backend could incorporate \_chat\_history to add context (not shown explicitly, but possible extension). - Example Outcome: User: "Explain relativity simply." -> AI: "Relativity is a theory by Einstein that says time and space are linked... [detailed but simple explanation]."
2. Creating a New Task (AI-Assisted Task Generation): - Purpose: Make it easy for the user to automate a new function by describing it in natural language, which the AI will turn into a task file. - User Action: In the Tasks page (Agent Console), the user clicks on “AI Task Creator” panel. They enter a description, for example: “Monitor my Downloads folder and whenever a PDF appears, move it to the Documents/Reports folder.” - UI -> Backend (Task Generation): The UI sends this description to the POST /api/v1/task\_generation endpoint (AI Assistance router). - Backend Processing: The backend’s task\_generation handler will: - Possibly use a predefined prompt template for the LLM: something like, “You are a coding assistant. The user wants to automate a task described as: [user description]. Output a YAML configuration and Python code for this task.” - It then calls an LLM (likely a coder model if available, e.g., qwen2.5-coder or OpenAI GPT-4) to generate a structured response. It may use LangChain to format and parse the output. - The response might come as a multi-part output: a YAML front-matter and code body. For example:
name: AutoFileMover
triggers:
 - event: file\_created
 filter: "*.pdf"
 folder: "~/Downloads"
actions:
 - move\_file: 
 from: "{{event.path}}"
 to: "~/Documents/Reports/{{event.name}}"
---
# Code (if needed, e.g., for more complex logic)
Or the AI might directly produce a Python script that implements this logic using the built-in file tools. - The backend receives this output and validates it (using jsonschema or expecting a certain format). The governance engine might ensure the code is safe (no malicious commands). - Storing the Task: The backend will then create a new file in the tasks/ directory, e.g., tasks/task\_auto\_file\_mover.py. It writes the generated code (and possibly YAML metadata as comments or separate file). If YAML is separate, it might integrate it into the code or register it in the database. - There could also be an entry added to the tasks registry (in DB or memory) so that it’s immediately recognized by the running app. If the app monitors the folder via watchdog, the FileSensor might detect “new file created” in tasks/ and automatically emit an event that could cause the system to reload task definitions. - Backend -> UI Response: The API responds to UI indicating success, perhaps returning the generated task name and summary. - UI Confirmation: The UI likely shows the generated task to the user in a preview or editor for confirmation. The user can review the plan and code. If something looks off, they could edit it manually or regenerate. - Activation: Once confirmed, the task becomes active. It should appear in the Tasks list in UI. The user can now run it or attach automation triggers to it (like events or schedule, as in the description it already suggested an event trigger). - Example Outcome: The system creates task\_auto\_file\_mover.py. Later, when the user downloads a PDF, the event system will trigger this task which moves the file as specified.
3. Executing a Task (Manual Run): - Purpose: Allow user to manually run a task on demand and see results. - User Action: On the Tasks page, the user sees a list of tasks (including ones they created or bundled examples). They click “Run” on a specific task, say DailyReportTask. - UI -> Backend: The UI calls POST /api/v1/tasks/{id}/execute (or possibly a general execute endpoint with task name). It passes any required parameters (if the task expects input, though many might not). - Backend Processing: The API route handling execution does: - Look up the task definition by id or name – possibly find the Python function or class associated or load the module from tasks/. - Submit the task to the Automation Engine’s queue. If the engine is an async function, maybe something like await engine.run\_task(task\_name). More likely, it defers to a background thread so it can return immediately. - The Automation Engine (in engine.py) takes over: It creates an Execution record in DB (status = running), then spawns an agent to run the task. - The agent executes step by step, calling tools or code as needed. The execution is logged. - If the API call was synchronous, it might wait until completion to return a result. But usually, running tasks are asynchronous – the API might immediately respond with something like { "status": "started", "execution\_id": 42 }. - Backend -> UI Response: The UI receives the acknowledgement. It then can monitor the task’s progress. Perhaps the UI opens a modal or goes to Executions page for that run. - Live Updates: As the task runs, each step could be sent to the UI. If using websockets, the backend might push updates to a Reflex state (e.g., append a log entry). If not, the UI might poll an endpoint like GET /api/v1/executions/{id} to get the latest status and logs. The monitoring could be every second or triggered by the backend when done. - Task Completion: When done, the Execution record is marked success/failure with any output. If the task produced a result (like an answer or created a file), that info could be accessible. Possibly the task function returns a result which the engine captures and includes in the execution record. - UI Display: The UI shows “Task finished successfully” with timestamp, and might display the final output. For example, if DailyReportTask was meant to print a summary, the print output could be captured and shown. - Example Outcome: The user sees that DailyReportTask ran and perhaps output “Report generated and emailed to you.” If there was an error, they see a failure notification and can click to see logs (where e.g. a stack trace or error message is shown, such as “File not found on path X”).
4. Scheduled Task Execution (Automation via Scheduler): - Purpose: Run tasks automatically at defined times. - User Setup: Suppose the user has a task BackupTask that backs up files. They schedule it to run every night at 2 AM. - User Action: In the Automation page’s Schedules section, they create a schedule: choose BackupTask from a dropdown of tasks, and set time "02:00 daily". - UI -> Backend: The UI calls POST /api/v1/schedules with data like task: BackupTask, cron: "0 2 * * *" (cron expression for 2:00 every day) or a structured schedule object. - Backend Processing: FastAPI’s schedules route will: - Create a Schedule entry in the DB with the cron and task association. - Add a job to APScheduler. It uses the cron expression to schedule and the job function references the task ID. - Confirm with a response containing the schedule ID and next run time. - At 2 AM: The scheduler triggers the job: - The job function enqueues BackupTask to the engine (like a manual run but initiated internally). - The task runs exactly as in manual execution flow above, headlessly. - If the system is not running at that moment, obviously it won’t run (this is local, not a cloud service). - User Notification: Because this runs unattended, how does the user know it ran? Several ways: - Next time they open the app, on the Dashboard it might show “Last backup: succeeded at 2:00”. - The Executions page will have an entry for the run at 2:00 with status success. - If a task fails or if notifications are configured, the system might send an alert (perhaps an email or just an on-screen alert next time user logs in). - The Monitoring page’s quick stats might include “Tasks run in last 24h: X”. - Example Outcome: BackupTask runs at 2 AM, backing up user’s data. The user checks next day: sees an execution record at 2:00 with green success. If they click it, they see “Backup completed, archive saved as backup\_20251024.zip”.
5. Event-Driven Workflow (File Event -> Task): - Purpose: React to events (like files, emails, webhooks) in real-time. - User Setup: Let’s use the example: user wants to automatically process new emails. They have a task SummarizeEmailTask that reads new emails and summarizes them into a note. - User Action: In the Automation page’s Events section, they create an event subscription: Event Type = email\_received, Filter = maybe sender contains "boss@company.com" (so only boss’s emails trigger), Action = SummarizeEmailTask. - UI -> Backend: The UI calls POST /api/v1/events with event\_type, filter criteria, and target task info. - Backend Processing: FastAPI adds an entry in event\_subscriptions table with those details. - If the email sensor is implemented via polling: the system ensures an APScheduler job is checking emails periodically, or if Graph webhook is used, that’s already set. But for our scenario: - We likely have a simple loop or scheduled task every X minutes that uses IMAP to check for new unread emails. - Each new email triggers an internal event. For demonstration, assume every minute the system runs:
new\_emails = check\_imap\_for\_unread()
for mail in new\_emails:
 emit\_event(EventTypes.EMAIL\_RECEIVED, {"from": mail.from, "subject": mail.subject, "body": mail.body})
- Event Bus Matching: The event bus receives an EMAIL\_RECEIVED with payload. It scans subscriptions: - Finds the one where filter matches (say if filter was sender=boss, it checks payload['from']). - It sees SummarizeEmailTask is subscribed, so it calls engine.enqueue\_task("SummarizeEmailTask", context=payload). - Task Execution: SummarizeEmailTask runs. It likely uses the email content (provided via context or the agent might use EmailReadTool to fetch details). The agent summarizes the email text (maybe using an LLM prompt “Summarize this email: [body]”). Then maybe it uses a FileWriteTool or NoteTool to save the summary somewhere or sends it to the user somehow (maybe via email reply or just logs it). - User Notification: If the user interface is open at that time, they could see a notification pop up: “Event: New email from boss - SummarizeEmailTask executed.” Possibly the UI’s notification system (Reflex can show a toast message if state updates accordingly). Or it could simply silently log and next time user checks they see it in Executions. - The result (summary text) could be stored as an artifact, or even emailed back to user. But let's say it stores in a file "boss\_email\_summary.txt". - Example Outcome: The user doesn’t have to do anything; whenever an email from boss arrives, a summary note is ready. They open the app later and see under Executions or maybe in a dedicated “Summaries” page the content generated.
6. RAG Document Query Workflow: - Purpose: User asks a question and system gives an answer with info from their documents. - User Action: On the RAG page, the user uploads a few PDF reports. After indexing (which they triggered via a button “Build Index”), they enter a query: “What were the Q3 sales figures for product X?” - UI -> Backend (Upload): The UI would call POST /api/v1/rag/documents/upload for each file, sending the file bytes. The backend saves them to docs/ folder. It responds “ok, uploaded, please index.” - Indexing: The user then triggers indexing. Possibly the UI calls an API to start indexing (maybe something like POST /api/v1/rag/index if exists) or it instructs the user to run a script (but better via API). - If via API, the backend will iterate over files in docs/, use sentence-transformers to embed them, build the FAISS index, and save index files (this can be time-consuming, so might be done in a background task with a progress indicator). Alternatively, the user runs rag\_index.py manually outside the app (less user-friendly though). - The system could also have auto-index: the FileSensor sees new files in docs/ and triggers an indexing event (maybe it enqueues a special indexing task). - Query: - UI calls POST /api/v1/rag/search with the query text. - Backend receives it, calls query\_rag(query, top\_k). This uses the FAISS index to find top matches and returns a list of results with snippets. - The results come back to UI as JSON: e.g.,
{ "ok": true, "results": [
 {"path": "Q3\_report.pdf", "snippet": "Product X sales in Q3 were $5M ...", "score": 0.95},
 {"path": "Q2\_report.pdf", "snippet": "Product X sales growth ...", "score": 0.90}
 ], "query": "...", "total": 2 }
- UI displays these snippets, highlights, and maybe the user can click them in the Document Viewer to see more context. - Answer Generation: - Perhaps the system also provides a direct answer. Two approaches: 1. The UI now takes those snippets and calls the Chat API (or a special RAG answer API) including the snippets in the prompt: “Using the info: [snippet1] ([doc name]) ... [snippet2] ... answer the question: [user query].” 2. Or there’s an endpoint like POST /api/v1/rag/answer that internally does retrieval + LLM in one go. - Let's assume the UI handles it by calling Chat endpoint with a combined prompt. The Chat endpoint then generates an answer, which likely references the documents. - The answer is returned and UI shows it in the center panel, with citation markers (because the prompt could have been engineered to include reference tags, or the system might post-process to add references). - UI Interactions: The right panel automatically opens Q3\_report.pdf and Q2\_report.pdf with highlights on the sentences that were used (matching maybe using the snippet text to find location). - Example Outcome: The user sees an answer: “Product X sales in Q3 were $5 million, as reported in Q3\_report.pdf. This was a 20% increase from Q2.” The references [**] are clickable and scroll the document viewer to those lines in the PDF.
7. Multi-Agent Collaboration Example: - Purpose: Solve a complex problem by having multiple specialized AI agents discuss. - User Action: On the Agents page, the user enters a prompt: “Design a simple web application that uses a database and explain the reasoning.” and selects “Use Multi-Agent (Team of 4)” mode. - UI -> Backend: Calls POST /api/v1/multi\_agent/session with the prompt and maybe a preset of agent roles (if not default). - Backend Processing (AutoGen): The multi\_agent router uses pyautogen to create a session with, say, 4 agents: a “Project Manager” agent that coordinates, a “Developer” agent for code, a “QA” agent to ask questions, and a “DevOps” agent to suggest deployment. - The Autogen framework will spawn four LLM contexts, and start a loop where they take turns sending messages. For example, the Project Manager agent receives the user request and says “Alright team, we need to design... Let’s break down tasks.” The Developer agent might propose a tech stack, the QA agent might raise concerns, etc. This is all AI-driven. - The orchestrator might run a fixed number of iterations or until agents converge. It collects the conversation. - Eventually, a final answer or design is produced (maybe the Project Manager agent summarizes the plan). - Streaming to UI: While this multi-agent conversation is happening, the UI could update in real-time, showing each message from each agent as if it’s a chat (with the agent’s role name as the speaker). This likely uses websockets or the UI polls for new messages in the session via GET /api/v1/multi\_agent/session/{id} repeatedly. - Completion: The final result (a design description) is returned and the session is closed or kept for review. The UI could allow the user to scroll through the whole dialogue or just see the result. - Example Outcome: The user sees a conversation log: - ProjectManager: “We need a web app with a DB. Developer, what do you suggest?” - Developer: “Use Flask with SQLite for simplicity.” - QA: “Will it handle concurrent users?” - Developer: “SQLite might not, maybe use Postgres for production.” - DevOps: “We can deploy on Heroku, which provides Postgres easily.” - ProjectManager: “Alright, the design is: a Flask app with a Postgres database, deployed on Heroku. It meets requirements and is easy to set up.” The final summarized answer is then presented by ProjectManager agent, and that is the output shown clearly.
8. System Monitoring and Self-Healing Example: - Purpose: Demonstrate the system’s internal monitoring and potential self-correction. - Scenario: Suppose an external API key (OpenAI) is not set, and user tries a cloud model chat. The request to OpenAI fails (HTTP 401). - Monitoring Reaction: The error handler catches the exception and logs “OpenAI API error: unauthorized” and the chat endpoint returns a 503 with detail "Ollama service unavailable" (the code in chat.py specifically handles httpx.HTTPError as Ollama unavailable, which might cover this case too by mistake). - Health Check: If the user checks the Health page, it might show openai\_api: error or similar warning that no API key. - UI Indication: The chat UI could show an error toast: “AI model service unavailable. Check API key or network.”. - Self-Healing: Not much to do except inform the user to fix config. But for certain issues, say GPU memory low: - The system could preempt new tasks (the scheduler might delay launching a new heavy task if GPU is at 100% usage). - Or if a task fails due to out-of-memory, the system could automatically restart the Ollama server or unload some models to free memory (like an agent can detect OOM and call an internal tool to clear cache). - Resource Management: If multiple tasks are queued and system is overloaded, the engine could throttle starting new tasks until usage goes down. This is more design consideration, but something an engineer could implement given the hooks we have (metrics and queue).
Through these breakdowns, it's clear that each feature flows logically: User action -> UI event -> API call -> backend logic -> AI model/tool operations -> results -> UI update. By following these workflows, developers can trace the path of data and identify where to implement or modify behaviors. Each module interacts through well-defined interfaces (HTTP calls or event emits), which makes the system robust and easier to reason about.
Security & Configuration Notes
Security is an important aspect even for a local application, especially if it interfaces with external accounts (email, APIs) or if it could be exposed on a network. Configuration management is also crucial for flexible and safe deployments. Here we cover how the system secures access, handles sensitive data, and how configuration is safely managed.
Authentication & Access Control: By default, the system can be run as a personal, local app without login prompts. However, the FastAPI backend includes a token-based authentication mechanism for API requests: - The environment variable API\_AUTH\_TOKEN serves as a master token for accessing the API. If this is set, the backend secures critical endpoints with the verify\_token dependency. The Reflex UI is configured to automatically include this token (for instance, stored in Reflex state or as a cookie) in its requests. This prevents unauthorized scripts or users on the same network from calling the API. - In a typical local scenario, the risk is low since it's all on localhost. But if the user port-forwarded or ran on a server, the token ensures only those who know it (like the user) can use the system. - Reflex’s session management: Reflex can manage user sessions and could be extended with login forms if multi-user was needed. Currently, it likely operates as single-user. If multi-user support were added (e.g., a family or team sharing an instance on a LAN), one could implement role-based access (RBAC) as hinted for future. For now, the assumption is full access to whoever can open the UI. - The api/auth.py may contain an OAuth2 scheme for token auth (if they considered implementing login pages), but since instructions said no explicit auth beyond token, we keep it simple.
API Exposure: The API by default listens on 127.0.0.1 (localhost), meaning it's not accessible from other machines unless changed. This is a security measure to keep it local. In production, if one wanted remote access, they'd host it behind HTTPS and require authentication. Nginx in the suggested production architecture would handle TLS and could do additional access control (like HTTP basic auth or IP filtering). - The user should keep the API token secret. The UI likely stores it in memory rather than in localStorage (Reflex might not expose browser JS storages since state is managed on server). - All state and operations assume trust in the user, as it's local. If needed, one could add permission layers for potentially dangerous actions (like running system commands) but in our design, the agent tools themselves define what's allowed (and presumably don't include an arbitrary shell exec tool by default, unless the CodeAgent runs Python code which could do anything — in that case, it's the user's responsibility not to run untrusted tasks).
Sensitive Data Handling: The system deals with: - API Keys and Secrets: (OpenAI keys, etc.) These are never hard-coded and not logged in plain text. They reside in .env and are loaded into memory. The configuration file config.yaml might reference keys indirectly (like a flag openrouter.enabled: true but the actual key is only in .env). The devs should ensure not to accidentally include secrets in logs or error messages. For instance, when logging config, skip or mask secret fields. - User Documents and Emails: These may contain personal or confidential information. The data stays on the local machine (in docs/ folder for files, in memory for emails fetched via IMAP). When the AI processes them, it's using local models or trusted APIs. If using a cloud model, then indeed content is sent to OpenAI or similar, which has privacy implications. This should be documented so the user is aware (e.g., “If you use OpenAI API, your prompts and data will be sent to OpenAI’s servers, so consider your company’s data policies”). For maximum privacy, use only local models. - Database encryption: SQLite by default is not encrypted. If the data is sensitive and the machine is multi-user, one could encrypt the DB or at least protect the filesystem (but since it's local, typically OS user permissions suffice). Future enhancements might integrate an encrypted SQLite or use the OS keychain for certain secrets.
Environment Variables & Secrets Management: - It's recommended to store all secrets in the .env file (which is gitignored). For deployment, the .env should be populated through a secure method (like an installer asking for API keys, or user manually editing). - The config.yaml might contain default values that are not sensitive (like default ports, toggles). But anything like credentials should go in env vars. - The documentation for deployment should remind to never commit .env to any repo. - If distributing an .exe, careful: environment variables might not be used in the same way. Possibly the .exe could read from a config file or prompt user for keys on first run, then store them in a config (maybe the Windows registry or a local encrypted store). For now, we assume advanced end-users who can edit a .env.
Safe Defaults: - Many features are opt-in: e.g., multi-agent (since it’s heavy, it's probably disabled unless configured), openrouter integration is off unless key provided, Outlook COM only if on Windows and pywin32 installed. - The system likely starts with local model and only switches to cloud if user explicitly does so. This avoids accidental data leaks to cloud. - The Reflex UI design tokens and theme emphasize accessibility but also indirectly security by preventing common vulnerabilities like XSS – since all UI is generated server-side and not taking arbitrary HTML from users (except maybe email content which is HTML but likely sanitized or displayed in an iframe/safe manner). - FastAPI’s default includes protection against CSRF for server-sent forms by not having any forms (we mostly use JSON API). Also, CORS is restricted to known origins (localhost only by default).
Error Handling & Recovery: - If the system encounters an error (uncaught exception), the middleware catches it and returns error response without exposing internal stack trace to the UI (to avoid leaking file paths or data). - The server logs the stack trace to the error log for developers. - For recovery: - If a config is malformed, e.g., YAML syntax error, the app might fail to load. They may include try/except around config load to at least start with defaults and alert the user of config parse issue. - If the database is missing or corrupt, the system might try to create a new one or backup the corrupt one and start fresh (depending on how much they implemented; likely they rely on SQLite’s robustness). - Watchdog ensures if a file sensor fails, it logs but doesn’t crash the app thread. - The system doesn’t have an explicit “circuit breaker” for external services, but if OpenAI is failing repeatedly, the user can disable that integration.
Secret Management for Email: Storing email password in .env is fine for local personal use. More secure would be OAuth for Gmail, but that’s complex (though MS Graph was aimed at that). Given “we will use imap config only”, it implies user will provide email credentials. The system should ensure these are not logged (e.g., if an IMAP login fails, catch the exception and log a generic message rather than the actual password or so). If storing in config.yaml, that’s plain text on disk—prefer .env.
Access Control for Local vs External APIs: The app exposes two main services: - The Reflex frontend (port 3000) – by default accessible to anyone on local network? Actually, Reflex likely binds to 0.0.0.0 or 127.0.0.1 depending on config. In dev, it might be 0.0.0.0 which means if you’re on a network, others could load the UI. We should ensure that in production, one restricts it (or at least rely on token auth). - The FastAPI (port 8502) – as noted, bound to localhost by default, which is safe. If needed for remote, user must intentionally configure host and should secure with token + ideally HTTPS. - If packaging as .exe, presumably running on a user's PC, not a server, so network exposure is minimal.
Updates & Versioning: - When updating the system (pulling a new version of code or exe), ensure backwards compatibility: - Database migrations if any (none mentioned, but if going from v1.0 to v1.1 and DB schema changes, provide a migration or at least auto upgrade). - The Appendix on versioning (below) likely covers maintaining a version number (as seen in architecture doc, version 2.0 post-recovery). This helps in logging and user reporting issues. - Ideally, have a check (maybe via a GitHub release feed) to notify user if a newer version is available, but only if internet is allowed. - The design docs mention "Post-Recovery" which implies there was a failure and recovery process. Possibly the system can detect an unclean shutdown or a crash and go into a safe mode. E.g., if tasks were mid-run, mark them aborted in DB on next startup to avoid confusion.
Configuration Validation Mechanisms: - The system should validate config.yaml content: use Pydantic or simple checks to ensure required sections and fields are present. If something is missing, fill with default or warn. - Same for .env: if a needed key (like OPENAI\_API\_KEY when cloud is enabled) is not present, the system can warn on startup (“OpenAI enabled but no API key provided”). - The UI Config page (if exists) might show status of keys (like in outlook\_health it lists which integrations are configured or available). - If the user toggles a config via UI (like enabling openrouter), the system might immediately try a test call (maybe list models) to verify and then confirm “OpenRouter configured correctly” or show error to re-check the key.
In summary, security in this local agent focuses on not exposing it unnecessarily, protecting user secrets, and providing an optional auth token for API access. Meanwhile, configuration is made flexible through .env and YAML, with safe defaults so that a user can run the system with minimal initial setup (maybe just installing and running with local model and no external integrations, which should work out-of-the-box with no keys needed). Documenting these aspects ensures that developers set them up correctly and users operate the system without compromising their data.
Appendices
Glossary of Key Modules and Classes
Task: A user-defined unit of work, typically implemented as a Python script (in the tasks/ directory) that performs a specific automation function. Tasks can be run manually or triggered by schedules/events. They often correspond to an AI agent execution.
Workflow: A multi-step process consisting of multiple tasks or actions executed in sequence or based on logic. Defined in the workflows system (either via YAML or through the UI workflow builder), managed by the workflow engine. Allows more complex automation than a single task.
Agent (AI Agent): In this context, an AI-driven entity (powered by an LLM) that can perform reasoning and use tools to accomplish a task. For example, a CodeAgent that can execute Python, or a ToolCallingAgent that can invoke web search, file read/write, etc. Agents are created by the system when running tasks, using the SmolAgents library.
SmolAgents: A lightweight framework for defining agents and tools. It manages the prompt loop with the LLM and interprets the LLM outputs to decide when to use a tool or provide an answer. In our system, SmolAgents is the backbone for most single-agent task executions.
AutoGen (Microsoft Agent Framework): A multi-agent collaboration framework allowing multiple AI agents to converse and solve problems together. Integrated as the pyautogen library, used for the Multi-Agent feature where roles like researcher, coder, etc., collaborate.
LangChain: A library providing utilities for chaining LLM calls and integrating with data sources. Used here to support things like retrieval (RAG chain), structured output parsing, and possibly memory. It's not the primary agent loop (SmolAgents is), but supports ancillary AI tasks.
LangGraph: A library to impose a graph structure on LLM interactions. In our system, it's part of the AI Governance mechanism (ai\_gov module) ensuring that the agent's reasoning follows certain predefined steps or templates (represented as nodes/edges). Think of it as a way to guide the LLM to produce output in a controlled format.
AI Governance Engine: The subsystem that oversees AI agent behavior for compliance and structure. It may validate outputs (with jsonschema), enforce policies (no disallowed content), and route tasks to different models or prompts depending on context. The governance router and ai\_gov code implement these rules.
Event Bus: The component that receives events from sensors and dispatches them to any subscribed automation. Defined in automation.events. It keeps track of subscriptions (from event\_subscriptions table) and when an event like FILE\_CREATED or EMAIL\_RECEIVED is emitted, it matches and triggers the associated task/workflow.
Sensor: A watcher that detects external changes and generates events. Examples: FileSensor (monitors filesystem changes), EmailSensor (monitors incoming emails), Webhook listener (HTTP endpoint that translates to internal events). Sensors decouple detection from action – they only emit events; the Event Bus decides what to do.
Scheduler: The APScheduler-based job scheduler that runs tasks on a time-based schedule. It reads schedule definitions (cron expressions or intervals tied to tasks) and ensures tasks are enqueued at the right times.
Execution Record: A database entry that logs the run of a task or workflow. Contains info like execution\_id, the task name, start/end times, status, and possibly a reference to logs or outputs. Allows reviewing past runs.
Artifact: An output file or data produced by a task execution. For instance, a PDF report generated by a task would be an artifact. The system tracks artifacts in the DB (with pointers to file path) so the UI can list or download them.
Reflex (Frontend Framework): A Python-to-Web framework (Pynecone) used for the UI. Key classes include:
rx.Component: A building block of UI (like a Panel, Button, etc.)
rx.State: Holds UI state and defines event handler methods.
AppState: The global state class for the app (in state/app\_state.py), containing things like current user, theme, notifications, and maybe the API token.
PageState subclasses: e.g., RagState, ChatState that hold page-specific data (like query text, search results for RAG; or chat history for Chat page).
api\_client: A helper service in Reflex that wraps calls to the FastAPI (using httpx). e.g., api\_client.get\_health() might call GET /api/v1/health and return parsed data.
Design Tokens: Predefined constants for UI styling (found in ui/theme.py as color palette, spacing sizes, font sizes, etc.). Ensures consistent styling across components.
AppShell / Layout Components: These are specialized UI components that frame the pages. AppShell likely includes the sidebar (components/sidebar.py), header (with breadcrumbs from NAV\_CONFIG), and content area. It’s used on each page to provide consistent navigation and structure.
NAV\_CONFIG: A data structure listing all pages and their metadata (icon, route, title). Possibly in sidebar.py or separate config, used to generate the sidebar menu and breadcrumbs dynamically.
Tool (AI Tool): A capability that an agent can use to perform actions. Tools are implemented as Python functions (and in SmolAgents, usually have a name and a description so the LLM knows when to use them). Examples: search\_documents(query) as RAG tool, read\_file(path) as file tool, etc. They enable agents to interact with the environment beyond pure text. Tools are declared in agent initialization (the agent is told what tools are available).
LocalVectorStore (FAISS wrapper): A class in rag\_query.py that wraps the FAISS index and provides a similarity\_search(query, k) interface. It abstracts whether using FAISS directly or a fallback (like if FAISS not present, uses an alternate method).
Local Models vs Cloud Models: Terminology for whether the LLM is running on the user's hardware (via Ollama or other on-prem runtime) or being accessed through an API (OpenAI/OpenRouter). The system often toggles a flag or base URL to switch between these.
OpenRouter: A service that routes requests to various free AI models via a unified API (needs an API key). We use this name to denote using remote models from providers like OpenAI, but through OpenRouter's gateway. It's configured in config and used when user selects "Cloud (OpenRouter)" mode.
GPUtil / psutil: Libraries, but often referred to as part of “Monitoring”. GPUtil deals with GPU info, psutil with system info. You may see references like GPUMonitor or similar if they had wrapper classes, but essentially it's direct usage.
EventTypes (Enum): An enumeration of possible events (like FILE\_CREATED, FILE\_DELETED, EMAIL\_RECEIVED, WEBHOOK\_RECEIVED, TASK\_FAILED, etc.). Used by sensors to tag events, and by subscriptions to identify them.
Outlook Tools / IMAP integration: While we removed Outlook COM emphasis, you might see references to "Outlook tool" meaning email integration tools that now use IMAP. Also, in code, outlook\_graph meaning MS Graph API usage. For glossary: Outlook here generically means email integration, and specifically:
Outlook COM: Windows COM automation for Outlook (optional).
Outlook Graph: using Microsoft Graph API for O365 mail (optional).
Generic IMAP: connecting to mail server via IMAP/SMTP (default cross-platform method).
Health Check (HealthResponse): The aggregated status of system components returned by /health. It likely includes fields like database\_ok, disk\_free\_gb, memory\_used\_percent, ollama\_running, email\_integration: "imap\_available" etc. The HealthResponse Pydantic model defines these fields.
Understanding these terms and components will help developers navigate the codebase and architecture when implementing new features or debugging issues.
CLI Command Reference
Developers and advanced users can use the following command-line interface (CLI) commands and scripts to operate and manage the system:
Running the Backend Server:
uvicorn api.app:app --reload --host 127.0.0.1 --port 8502 – Launch the FastAPI server in development mode with hot-reload. This will serve the API at http://127.0.0.1:8502.
uvicorn api.app:app --host 0.0.0.0 --port 8502 – Run the API in production mode (no reload, no debug). Use host 0.0.0.0 to listen on all interfaces if external access is needed (ensure to secure it).
Alternatively, if start\_api.py exists: python start\_api.py – Possibly a convenience script doing the above.
Running the Reflex Frontend:
cd reflex\_ui && reflex run – Start the Reflex development server (which builds the frontend and serves it at port 3000, with backend at 8000).
Options:
--env prod – run in production mode (minified assets, no debug).
--frontend-port 3000 --backend-port 8000 – specify ports if needed (should match config and CORS).
reflex export – build a static bundle of the frontend (if one wants to serve it separately, though Reflex always needs a backend for state unless redesigned).
Unified Startup:
./start\_all.sh – A shell script provided to start both API and UI together (optimized for local run). It likely:
Activates venv, ensures Ollama is running, maybe sets WAL mode on SQLite, then runs uvicorn and reflex. Possibly it runs Streamlit if it was legacy, but in v1.5 context it might have been doing that. If adapted to Reflex, ensure it runs both processes appropriately (maybe concurrently via tmux or background processes).
./run.sh --profile assistant – This appears to start a CLI version of the AI assistant (not the GUI) with a certain profile (maybe different chain setups). Profiles could be:
assistant – a general chat assistant in terminal.
coder – a coding assistant (maybe uses coder model).
visual – possibly a vision model in terminal (if qwen-vl). This is a more experimental interface, possibly using smolagents to handle queries via command-line.
Building Index for RAG:
python rag\_index.py – Process files in the docs/ directory and build the FAISS vector index. This should be run whenever new documents are added or existing ones change (unless auto-indexing is turned on). It outputs index.faiss, meta.jsonl, etc., typically in an ./index or ./out/rag directory.
python rag\_query.py "Your query here" – (If implemented as CLI) This could allow querying the index from CLI for testing. It would print out the top results snippets.
Testing Commands:
pytest – Run all tests. Add -s to see print/log output, -k "keyword" to filter tests by keyword.
pytest tests/test\_tasks.py::test\_some\_function – Run a specific test function.
pytest --headless (if UI tests exist that use a browser, might not apply here).
ruff . or flake8 – If a linter is configured (we saw references to pre-commit and ruff config), run code lint.
mypy . – If using static type checking (some type hints appear, but not sure if enforced).
Maintenance and Utility Scripts:
scripts/backup.sh – Creates a backup of key data. Usually usage: bash scripts/backup.sh and it would output a file in perhaps backups/ directory with timestamp. Ensure no running tasks while backing up to get consistent DB copy (or use .backup command of SQLite via script).
scripts/restore.sh – Restore from a backup archive. Usage: bash scripts/restore.sh path/to/backup.zip. Make sure to stop the app before restoring to avoid conflicts.
scripts/install\_faiss\_gpu.sh – Installs FAISS GPU via conda or pip; run if you have a GPU and want to leverage it for faster indexing/search.
Possibly scripts/ui\_audit.py – from the docs, does a UI consistency check (for design tokens, etc.), used during development to ensure no stray styles.
scripts/migrate\_icons.py – might be a one-time script to update icon usage in the code (from the UI revamp).
There might be a CLI to add a user or such if user accounts were implemented, but since not, there's none.
Model Management Commands:
ollama pull  – not a Python CLI, but important for setup, as documented. Example: ollama pull llama2:13b.
python -m utils.ollama\_config – Possibly tests connectivity to Ollama (maybe prints available models or checks version). Good for verifying WSL IP config.
If wanting to run local models outside Ollama (like directly via Transformers), no direct CLI but one could write a small test to load a model to ensure PyTorch is working.
Reflex UI CLI (via reflex command):
reflex init – (already done in project) initializes the Reflex app (generates template files).
reflex deploy – Use Reflex's deployment if you want to host the UI on Reflex cloud (not likely here) or build static assets. Usually, reflex deploy triggers build and can optionally push to a hosting platform.
reflex --help – Lists available subcommands in Reflex CLI.
Database CLI:
Direct SQLite access: sqlite3 unified\_system.db – open the DB to inspect tables manually (for debugging or data migration).
One can run SQL queries like SELECT * FROM executions; to see history, etc. (Be cautious not to modify unless you know what you're doing).
If migrating to Postgres, then standard psql or adminer usage would apply, but currently using SQLite.
Docker Commands (if using Docker):
docker compose up -d – Bring up the whole system with all containers (assuming a docker-compose.yml is provided as per architecture).
docker compose logs -f api – Follow logs of the API container.
docker compose down – Stop the containers.
For a single container run (not recommended for full system, but if testing API only): docker build -t local-agent-api -f Dockerfile . then docker run -p 8502:8502 local-agent-api.
Ensure to run these commands in the correct directory (as noted, often need to cd reflex\_ui for UI commands). The CLI tools and scripts provide both a means to operate the application and to perform auxiliary tasks (like indexing and backup) outside the UI. They are especially useful for developers or when automating deployment.
Dependency Graph Overview
Understanding how different parts of the system depend on each other is crucial for making changes or diagnosing issues. Below is an overview (in prose form, with references) of the dependency graph and interactions:
Frontend-Backend: The Reflex UI depends on the FastAPI backend for all dynamic data and actions. It communicates exclusively via HTTP requests and WebSocket state updates to the FastAPI:
Reflex UI (port 3000) -> makes API calls to FastAPI (port 8502) for data (e.g., to get tasks list, run a task, query RAG).
FastAPI sends responses which Reflex uses to update UI state.
Reflex’s backend (port 8000) acts as an intermediary, but essentially just proxies calls (through httpx) to FastAPI and manages UI state.
This means if FastAPI is down, the UI will not function (it might show errors or not load certain data).
Conversely, FastAPI doesn’t depend on UI – it can run and serve API clients (even external ones or CLI) without the Reflex frontend.
Backend Internal Dependencies:
FastAPI -> Automation Engine: When certain endpoints are called (execute task, schedule event, etc.), FastAPI functions will call into the automation engine (in automation/engine.py and related modules) to perform the requested action.
For example, tasks.execute endpoint might call automation.engine.run\_task(task\_id).
The engine then depends on Agents (SmolAgents) to run the task logic.
Agents (SmolAgents) -> LLM provider: SmolAgents will call either the OpenAI API (through openai lib) or the local model (through httpx to Ollama) depending on configuration.
SmolAgents also uses the set of Tools. Each tool might depend on other libraries:
RAGSearchTool -> depends on rag\_query.py which depends on FAISS and numpy.
File tools -> depend on Python I/O and possibly external programs if editing docs (but here likely just standard Python file access).
Email tools -> depend on win32com (pywin32) if Outlook COM, or imaplib/smtplib if IMAP, or msal+requests if Graph API.
Web tools -> depend on ddgs for search and requests or httpx for fetching webpages.
Office tools -> depend on python-docx, openpyxl to manipulate documents.
Thus, an Agent might indirectly use any of those libs when executing a step.
Agents (especially CodeAgent) depend on the Python execution environment. A CodeAgent might exec() Python code in a sandbox (which has access to the same environment, so it can use any installed library – there's a risk if not sandboxed, but given it's local user code, it's intentional).
APScheduler (Scheduler): The scheduler is initialized in the backend (perhaps in automation.startup). APScheduler itself depends on system time and its own thread or AsyncIO job.
It depends on the database for persistence (if jobs are stored there) but likely the schedules are loaded from DB each start and not persisted by APScheduler beyond memory.
When a job triggers, APScheduler calls a function in our code, which then goes back to the engine to enqueue a task, or directly calls an API route internally.
Event Bus: Depends on sensors to feed it events. Sensors like FileSensor run in their threads and call automation.events.emit\_event(). So the Event Bus receives data from sensors (FileSensor depends on watchdog).
The Event Bus then depends on the engine (to enqueue tasks) and database (to lookup what to trigger for a given event type, via event\_subscriptions).
Database (SQLAlchemy): Many components depend on the database:
FastAPI endpoints read/write it for tasks, schedules, executions etc.
Engine logs execution steps to DB.
The Monitoring health check might query DB (for e.g., number of records or last execution time).
The Reflex UI might also query some things through API which hit DB (like listing documents just reads the docs directory instead of DB).
So, if the DB is locked or slow, many parts degrade. WAL mode and thread-local connections alleviate some issues.
Vector Store (FAISS): Used by RAG subsystem:
The rag\_index.py and rag\_query.py depend on faiss, numpy, and sentence-transformers (which in turn depends on PyTorch).
The retrieval tool and endpoints depend on the index files being present and accessible on disk.
If FAISS is not available (not installed), the system falls back to a dummy or uses LangChain's in-memory as backup (the code tries an import and throws if not found).
External Services Integration:
Ollama: The local model server. The backend depends on Ollama being running for any local model requests. If it's down, those requests fail and error out as we saw in chat endpoint catch block. Ollama itself depends on the models being pulled and on having enough system resources (RAM/VRAM).
OpenAI/OpenRouter: The backend's use of openai lib or OpenRouter API depends on internet connectivity and correct API keys. If network is down or key invalid, calls will raise exceptions, which we handle by returning errors to user.
IMAP/SMTP: The email integration depends on the mail server. If wrong credentials or network issues, those tools will fail (health check will show graph\_available false or warnings).
Microsoft Graph (if used): depends on MSAL tokens. Without proper setup, is\_graph\_available() will be false, and system will just warn. Not critical if using IMAP instead.
Inter-module Relations:
api.routes.tasks and api.routes.workflows likely both use the engine under the hood. The tasks route for execution, workflows route for either execution or building DAG.
automation.engine uses smol\_agent\_factory to create agents (which presumably uses SmolAgents library to instantiate an agent with given tools).
smol\_agent\_factory likely chooses which model to use (reads config, if provider=ollama, sets base\_url accordingly, if openai, uses openai lib with api\_key from env).
The UI state classes often mirror backend models: e.g., there might be a Task model in backend (Pydantic) and a corresponding reflex State or data structure in UI to hold tasks.
monitoring.health.get\_health\_checker() might aggregate info from multiple modules: DB (via a simple SELECT 1 or checking file size), Ollama (ping via requests), sensors (maybe check if file sensor thread is alive), scheduler (APScheduler has API to get next jobs, can infer it's running).
monitoring.metrics\_exporter (if exists) collects metrics from various parts (it might register listeners in FastAPI to count requests, etc.). It likely relies on a global metrics object that increments counters on each request or event. The dependency is such that API endpoints might call metrics.log\_request() as part of middleware.
A simplified dependency graph in bullet form: - Reflex UI (frontend) --> FastAPI API. - FastAPI API --> Automation Engine (for tasks, events, schedules). - Automation Engine --> Agents (SmolAgents/AutoGen) --> LLM services (Ollama or OpenAI). - Automation Engine --> Database (for logging executions, reading definitions). - Agents --> Tools (file, web, email, etc.) --> OS and external services (filesystem, internet, email server). - Sensors (Watchdog, IMAP polling) --> Event Bus --> Automation Engine (trigger tasks). - Monitoring (psutil, GPUtil) --> Health endpoints --> Reflex UI monitoring page. - Config (config.yaml, .env) --> Almost every component (API uses ports and token from config/env, model provider from config, scheduler max\_workers from config, etc.) – it's the central configuration dependency.
One can see most arrows converge into the Automation Engine and Agents – that’s the core runtime, orchestrating tasks between inputs (schedules/events/UI) and outputs (files/emails/answers). The UI and external services are more on the periphery of the graph.
Versioning, Documentation, and Update Workflow
Versioning: The project adheres to semantic versioning for its releases (Major.Minor.Patch). The current system version is indicated in the FastAPI app (for example, version="1.0.0" in app.py), and also documented in architecture files (Architecture.md notes Version: 2.0 (Post-Recovery)). This implies: - Major versions (1.x to 2.x) introduce significant changes or overhauls (for instance, the migration from Streamlit UI to Reflex UI could have been the jump from 1.x to 2.0). - Minor versions add features and improvements in a backward-compatible way. - Patch versions fix bugs and minor issues without changing APIs.
The codebase might maintain a changelog file (perhaps CHANGELOG.md or as mentioned CHANGELOG\_UI.md for UI changes) which records notable changes per version. Developers should update the version string and changelog whenever changes are made. The UI footer or about dialog likely shows the version number for users to report when needed.
Documentation: The project includes extensive documentation in the docs/ directory and within code. Key documentation includes: - README.md (main) – Provides an overview, prerequisites, and quick start steps for the entire system. It is the first point of reference for a new developer or user. - GETTING\_STARTED.md – Step-by-step installation and initial usage guide. - ARCHITECTURE.md – A deep dive into system architecture, layers, data flow, and design decisions. This is crucial for new engineers to understand the high-level structure and rationale. - WSL\_SETUP.md, OPENROUTER\_SETUP.md, etc. – Specialized guides for particular environments or features (e.g., how to configure WSL with Ollama, how to enable OpenRouter models). - UI documentation in docs/ui/ – Contains specifics about the UI redesign, component usage, accessibility (like UI Playbook, Migration Guide, etc.). Useful for front-end developers to adhere to design system. - Status reports in archive/legacy-docs/ – These seem to capture historical context, decisions, and analyses (like console flow analysis, fix plans, etc.). They are mostly for reference to see how things evolved or why certain tech choices were made (for example, why move to Reflex from Streamlit). - Inline code documentation: The code is written with docstrings for classes and methods (we saw docstrings for health checks, sensors, etc.), and often comments above important logic describing why (like in config loading, try/except to provide defaults). It's important to maintain these docstrings and update them when code changes.
The documentation should be kept up-to-date as the system evolves. Each major feature should be reflected in the docs: - If a new module is added, include it in the architecture doc and update diagrams. - If a config option is added, mention it in GETTING\_STARTED or a Config Reference doc. - The README should have the simplest instructions; deeper down docs can have the intricate details.
Update Workflow: - The system likely uses git for version control. The update workflow for developers: - Create a feature branch, implement changes, run tests, update documentation and version if needed, then merge. - Possibly uses issues and PRs (if on GitHub or similar) to track changes – the presence of a COMPREHENSIVE\_CODE\_REVIEW\_REPORT suggests thorough reviews are done. - For end-users updating the deployed application: - If using a packaged .exe, the team might periodically release a new installer or executable. The user would be instructed to backup data, then run the new installer which overwrites the binaries but keeps user data (maybe stored in a separate folder). - If using Docker, they'd pull the new image and restart containers. The compose setup should mount volumes for persistent data (like the SQLite DB and docs directory) so those remain intact across updates. - If using pip (for a dev user), they'd do git pull and pip install -r requirements.txt again to get new dependencies, then run migrations if any (though currently, migrations are not complex). - The system should ideally detect if an update needs to upgrade the database schema. One simple strategy: keep a DB\_VERSION number in a config table. On startup, if code expects version 2 and DB is version 1, run migration script (SQL to ALTER tables or such), then update the version in DB. Because migrations are few, explicit code can handle it, or integrate something like Alembic if it grew more complex. - For example, if in v2.0 we add an alerts table, the startup code could check if that table exists; if not, create it on the fly and log a message. - Backward compatibility: Try to maintain backward compatibility for minor updates. For instance, if a new version of Reflex changed some component behavior, test and ensure our UI code is updated (like how they needed to fix deprecations in Reflex UI update). - The UI might have a clear separation such that it can be updated independently of backend as long as API endpoints remain stable. But usually, we update both together. That means version numbers cover both UI and backend code.
Continuous Integration/Testing: Possibly a CI pipeline runs tests on push to main. While not explicit, the presence of a thorough code review report suggests a disciplined dev process. A CI would lint (ruff), run tests (pytest), maybe build the UI to catch any compile errors, and perhaps even package the app to ensure no packaging issues.
Deployment Pipeline: If deploying to users, perhaps: - For .exe: use PyInstaller to produce an artifact for each new version. Test it on at least Windows 10/11. - For Docker: push images to a container registry with tags like local-agent:2.0.0. - For pip: if open-sourced, publish to PyPI, but this seems more internal so maybe not.
Maintaining Documentation: The development team should update: - This very master document (to keep it as a blueprint) whenever architecture changes or new modules are added. - The user-facing docs (README, etc.) for any changes that affect user steps. - Comments in code for any logic changes.
Communication of Updates: It might be useful to have a "Release Notes" that highlights new features or changes in each version. Possibly the CHANGELOG\_UI and maybe a general CHANGELOG serve that purpose. For example: - "v1.5: Introduced multi-page GUI with Reflex, added GPU monitoring, integrated OpenRouter for free cloud models." - "v2.0: Major UI redesign with design tokens and modern components, replaced Outlook COM with IMAP, improved concurrency and caching."
Post-Recovery Note: The architecture doc mentions "Post-Recovery" – likely meaning after a system crash or data loss, steps were taken to recover. Possibly they had to restore from a backup and made improvements. The design now includes backups and more robust logging. If something similar occurs, having documented procedures to recover (like how to restore the DB, how to fix broken index files by rebuilding) is valuable. Some of those might be in TROUBLESHOOTING.md or similar.
Finally, encourage contributions to documentation. If a developer finds outdated info or something unclear, they should update the docs as part of their change. This ensures the blueprint and guides remain accurate for future team members and users.

 ARCHITECTURE.md

 README.md

 README.md

 FINAL\_STATUS.md

 requirements.in

 app.py

 chat.py

 rag.py

 file\_sensor.py

 rag\_query.py

 outlook\_health.py
